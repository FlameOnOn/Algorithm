1、重点高校计算机或相关专业本科及以上学历，3年以上工作经验；              
2、精通Java编程，熟悉Java常用开源框架、设计模式及多线程开发，熟悉性能优化、对架构设计有一定理解和实践经验者优先；                  
3、熟悉MySQL数据库，有一定的SQL性能调优经验，对其内部原理有深刻理解者优先；              
4、熟悉Hadoop/MapReduce/Hive/HBase/Kafka/Spark等一项或多项大数据处理技术，具有较丰富的大数据开发经验；                  
5、熟悉Shell\Python等至少一门脚本语言；  

1、JAVA基础扎实，包括JVM、IO、多线程、并发、网络，深刻理解面向对象、设计原则、封装抽象等； 
2、熟练使用常用的Java技术框架，并对java web的各种开源框架如Spring、Strus，ibatis、dubbo等有深入的应用和优化经验，掌握它的原理和机制； 
3、熟悉分布式系统的设计和应用，熟悉数据库、缓存、消息队列、RPC等内部机制；


1. 扎实的计算机基础知识，精通Java 

2. 参与过较大规模软件项目开发，具备高效，健壮，简洁的设计和代码风格。      

3. 熟练掌握SQL语句，熟练使用关系数据库，熟悉数据库设计与性能调优    

4. 熟悉互联网开发模式，清晰理解缓存，缓存设计和模式；    

5. 有主流NoSQL(Mongo，Redis,HBase等)使用开发经验为佳    

6. 对Http ，TCP/IP 网络协议有清晰理解，        

7. 有高并发，高可用服务设计与研发经验为佳    

8. 熟悉Linux 开发环境，有 shell/awk/python等系统脚本编写能力为佳；   

1.具备扎实的计算机专业基本功,深厚的Java的编程基础,对Java内存模型、多线程、垃圾回收、NIO等有一定研究;

2.熟练掌握主流的开源框架设计理念，对Spring、Netty等有深入的了解、研究过源代码者优先;

3.熟练掌握基于Oracle或者Mysql的设计和开发，对zookeeper，memcache，redis，Kafka以及rpc有经验者优先; 

数据
3年以上JAVA开发经验，3年以上大数据相关经验。

本科以上学历，计算机及相关专业

具备良好的算法能力，扎实的代码编写能力，深入的理解分布式计算的原理和相关工具

熟练使用Hadoop，Spark，Kafka, redis等，熟悉更多的OLAP系统加分。

1.具有扎实的Java功底，对JVM的原理有一定的了解，具有较好的Java IO、多线程、网络等方面的编程能力； 

2.熟悉spring、MyBatis、Struts、Tomcat等常用Java开源框架，对其运行原理有较好的理解。 

3.精通数据库设计（Mysql优先），优秀的SQL编写及调优能力，熟悉常见NoSQL存储，如Hbase、memcached、redis、mongodb等； 

4.有大规模高并发互联网应用的设计和开发经验，熟悉常规的分布式架构，熟悉缓存、消息队列等开源中间件

1.3年及以上使用JAVA或者C++开发的经验，基础扎实，理解io、多线程、集合等基础框架。对JVM原理有一定的了解，对Spring,ibatis等开源框架熟悉；拥抱开源，喜欢阅读开源源码
2.具备优秀的架构设计能力；
3. 熟悉分布式系统的设计和应用，熟悉分布式、缓存、消息等机制；能对分布式常用技术进行合理应用，解决问题；
4. 掌握多线程及高性能的设计与编码及性能调优；有高并发应用开发经验；
5. 掌握Linux 操作系统和大型数据库（Oracle、MySql）；对sql优化有丰富的经验；
6. 学习能力强，适应能力好，有强烈的责任心，具备耐心/细心的品质；
加分项：
1.熟悉Lucene/Redis/Docker/Mesos/Hadoop等开源技术，开源社区活跃者优先；
2.有基于tensorflow/mxnet/caffe研发深度学习算法经验者优先。
3.有搜索、推荐、数据存储或者其他大型分布式系统的工程开发经历者优先；

2. 熟悉面向对象分析与设计，能编写高质、简洁、清晰的代码，熟练正确应用设计模式；
3. 具有大规模分布式系统应用架构设计与研发经验，熟悉底层中间件、分布式技术，对高可用、高性能有较深刻的认识；

3. 对Java EE服务器端设计和编程有深入理解，对ESB, Restful, Dubbo/HSF有深入了解 

3. 熟悉分布式系统的设计和应用，熟悉分布式、缓存、消息等机制；能对分布式常用技术进行合理应用，解决问题

1、计算机相关专业毕业，2年以上工作经验，有大数据高并发的处理经验，精通spring框架，阅读过spring源码，熟悉其事务机制
2、熟练应用Linux操作系统，熟悉GIT等版本管理软件
3、掌握网站访问速度的各种优化方案,并提出优化方案更佳
4、掌握Spring、SpringMVC、mybatis、Redis、JavaScript、CSS3、XML、AJAX等知识，能够灵活运用
5、熟悉软件设计流程和软件工程规范，具备编写良好而规范的设计和技术文档的能力
6、熟悉Oracle、sql server、mysql等大型数据库一种或多种，熟练编写SQL语句及sql优化

1.有良好的编码习惯及严谨的逻辑思维
2.具备数据库使用的基本知识：事务，索引，熟悉SQL语法，有初步的数据库建模知识
3.熟练掌握 Spring+Spring MVC，JQuery ，Mybatis 基本框架的使用
4.理解同步与异步的使用场景；了解悲观锁、乐观锁，并知道其区别与优缺点
5.理解redis缓存，MQ，任务调度（Worker）使用场景，能够使用以上工具完成编码任务
6.了解Mysql的特点，能够正确的使用mysql索引
7.掌握常见SQL的优化技巧
8.准确理解事务基本原理并能正确使用
9.理解面向对象的设计思想，了解常见的设计模式的使用场景
10.掌握单元测试的使用方法并在项目中熟练使用
11.熟练使用GIT代码管理工具。



高级
在中级基础上需具备以下要求：
1.对 Spring+Spring MVC，JQuery，Mybatis 基本框架有丰富的使用经验
2.数据建模经验丰富，能够独立完成单个项目的数据建模工作
3.能够跟踪诊断典型的线上问题
4.对多线程、事务有深刻的认识和丰富的使用实战经验；对高并发方案有自己的见解
5.掌握领域建模的理论与方法，并有实际项目建模经验

6、熟悉memcache、redis等缓存技术使用； 
、熟悉Linux操作系统和调优，熟练掌握idea开发环境；
3、熟练使用Spring，myBatis框架，阅读过源码优先；

1、良好的编码规范、乐于尝试新的技术
2、熟悉Linux系统，能够编码基本的SHELL
3、熟悉JVM、多线程、并发编程、JAVA8、函数编程
4、熟悉HTTP、Nginx了解DNS,TCP/IP
5、熟练使用Maven、Git、SpringBoot进行项目开发
6、熟练使用SpringMvc、Mybatis
7、了解JavaScrip,CSS

、 Java基础扎实、理解JVM原理、有多线程、并发，有广告系统开发经验的优先考虑；
3、 熟悉HTML5、JavaScript、CSS、JQuery等Web前端技术；
4、 熟悉主流开源应用框架，如Spring、struts、iBatis、hibernate、velocity、XML、JSON、Maven等开发技术；
5、 熟悉webservice、Dubbo、ZooKeeper, nginx等；
6、 熟悉MySQL等关系型数据库，以及相应数据库调优、SQL优化；
7、 有MongoDB、Redis、elasticsearch,hadoop、hive、pig、hbase、kafka、python经验优先考虑; 

2. 具备数据库使用的基本知识：事务，索引，熟悉SQL语法，有初步的数据库建模知识
3. 熟练掌握 Spring + Spring MVC，JQuery ，Mybatis 基本框架的使用
4. 理解同步与异步的使用场景；了解悲观锁、乐观锁，并知道其区别与优缺点
5. 理解redis缓存，MQ，任务调度（Worker）使用场景，能够使用以上工具完成编码任务
6. 了解Mysql的特点，能够正确的使用mysql索引
7. 掌握常见SQL的优化技巧
8. 准确理解事务基本原理并能正确使用；
9. 理解面向对象的设计思想，了解常见的设计模式的使用场景
10. 掌握单元测试的使用方法并在项目中熟练使用；
11.熟练使用GIT代码管理工具。 

1. Java基础扎实，理解io、多线程、集合等基础框架，对JVM原理精通；
2. 对于你用过的开源框架，熟知它的原理和机制；对Spring/Struts/Ibatis开源框架熟悉；
3. 熟悉分布式系统的设计和应用，熟悉分布式、缓存、消息、搜索\推荐等机制；
4. 掌握多线程及高性能的设计与编码及性能调优；有高并发应用开发经验；
5. 深刻理解企业应用设计模式，有大型分布式，高并发，高负载，高可用性系统设计开发经验；
6. 有多进程/多线程网关开发经验者优先；

熟悉MYSQL及SQL语言、编程，了解NoSQL, key-value存储原理；
熟悉MVC/RESTful的架构
熟悉Mysql数据库开发，熟悉NoSQL，如memcache/redis；
精通HTTP、TCP/IP协议，网络编程；

蚂蚁金服
1、Java基础扎实，具备三年以上的Java研发经验。
2、熟悉分布式系统的设计和应用，熟悉分布式框架、中间件、数据库等机制，能对分布式常用技术进行合理应用解决问题，有2年以上大型分布式系统研发经验优先。
3、具有大型电子商务网站、银行业核心系统、互联网金融系统研发经验、以及高并发、稳定性技术经验的优先。

1、扎实的java编程基础，熟练单元测试技术和TDD，精通Java EE、SOA、OSGI等相关技术；对各种开源的框架如Spring、Hibernate、iBatis等有深入的了解，对框架本身有过开发或重构者可优先考虑；
2、三年以上大型数据库如oracle使用经验，3年以上大规模高并发访问的Web应用系统设计和开发经验；
3、熟练掌握unix/linux操作系统，对常用命令运用娴熟，能够根据实际需要快速编写shell脚本；
4、具备良好的识别和设计通用框架及模块的能力，熟悉UML；

1、扎实的java编程基础，精通Java EE、微服务、消息中间件等相关技术；对各种开源的框架如Spring,消息中间件等有深入的了解；
2、3年以上大规模高并发访问的Web应用系统设计和开发经验，丰富的线上运维经验；
3、具备良好的识别和设计通用框架及模块的能力；
4、具有大型电子商务网站以及银行业核心系统、电信boss系统设计与研发经验背景的优先考虑；
4、具备数据和算法开发及应用经验者优先。

SOA  rpc
有丰富的Java搜索开发经验，如Lucene，ElasticSearch，Solr等。
RPC框架



职位描述：

工作内容：

1. 负责系统的研发，及时解决项目涉及到的技术问题；
2. 参与系统需求分析与设计，负责完成代码编写，接口规范制定，架构设计。

任职资格：

1. 扎实的Java编程基础，3-5年以上Java Web开发经验；

2. 熟悉主流开源应用框架，如Spring Boot、Thymeleaf、MyBatis、Git、Maven等开发技术；

3. 熟悉html、js、jquery、Ajax等前端技术；

4. 熟悉关系型数据库MySQL，SQL优化；

5. 熟悉ElasticSearch、Redis、mq等；

6. 熟悉Linux操作系统,能够使用Linux系统工具完成日常工作；

7. 很强的分析问题和解决问题的能力、团队协作和沟通能力；有强烈的责任心；

8. 熟悉分布式系统的设计和应用。



6.有Redis，Elasticsearch等实战经验优先；
4、熟悉微服务架构理念，熟练掌握Spring、Redis、Mysql、Zookeeper等

熟悉微服务架构理念，熟练掌握Spring、Redis、Mysql、Zookeeper等。



360
    职位描述：

    岗位职责：

    1. 根据360手机助手多个业务场景的需求提供高可用的推荐系统服务接口，并对推荐结果数据进行线上融合，确保数据准确性；

    2. 实时采集捕获用户的即时兴趣趋势，并基于用户实时兴趣集提供线上rank服务；

    3. 参与推荐系统整体架构设计和实现，负责推荐引擎各个核心模块的开发实施；

    4. 踏实严谨的工作态度，对系统设计实现的用心思考都是你的加分项。


    任职要求：

    1. 两年到三年java互联网服务端后台相关开发经验，有扎实的多线程，网络，对象序列化编程经验，负责过推荐系统或者广告系统后端架构开发经验优先考虑；

    2. 精通高并发，低延迟服务设计，熟练掌握常用开源框架，至少熟悉两种nosql类数据，如：mongo，redis，kafka，zookeeper，thrift等；

    3. 精通linux开发环境，对java程序的部署框架以及性能优化有一定经验，深入理解面向对象设计，熟练掌握常用的设计模式；

    4. 熟悉实时流计算引擎，storm或spark streaming者优先考虑。
	
	
	
	
    职位描述：阿里蚂蚁金服

    岗位要求:

    1.具有扎实的Java功底，对JVM的原理有一定的了解，具有较好的Java IO、多线程、网络等方面的编程能力； 

    2.熟悉spring、MyBatis、Struts、Tomcat等常用Java开源框架，对其运行原理有较好的理解。 

    3.精通数据库设计（Mysql优先），优秀的SQL编写及调优能力，熟悉常见NoSQL存储，如Hbase、memcached、redis、mongodb等； 

    4.有大规模高并发互联网应用的设计和开发经验，熟悉常规的分布式架构，熟悉缓存、消息队列等开源中间件。 

    5.热爱技术研发，具有快速学习能力；注重代码质量,有良好的软件工程知识和编码规范意识。 

    6.具有较好的沟通能力，思路清晰，善于思考，能独立分析和解决问题。 

    7.有强烈的责任心和团队合作精神，良好的抗压能力，心态积极，能主动融入团队

    8、如果没有心仪的事业群，我会帮你内推到匹配度高，同时hc紧急的岗位

	
	 LR、GBDT、SVM、RL、DNN、CNN、RNN
	 数值分析，凸优化
	 
	  线性代数：我的一个同事 Skyler Speakman 最近说过，「线性代数是 21 世纪的数学」，我完全赞同他的说法。在机器学习领域，线性代数无处不在。主成分分析（PCA）、奇异值分解（SVD）、矩阵的特征分解、LU 分解、QR 分解、
	  对称矩阵、正交化和正交归一化、矩阵运算、投影、特征值和特征向量、向量空间和范数（Norms），这些都是理解机器学习中所使用的优化方法所需要的。令人惊奇的是现在有很多关于线性代数的在线资源。我一直说，
	  由于大量的资源在互联网是可以获取的，因而传统的教室正在消失。我最喜欢的线性代数课程是由 MIT Courseware 提供的（Gilbert Strang 教授的讲授的课程）
	  ：http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/
	  
	  
	  
	  
	  
	  参与：马亚雄、吴攀

过去的几个月中，有几人联系我，诉说他们对尝试进入数据科学的世界，以及用机器学习的技术去探索统计规律并构建无可挑剔的数据驱动型产品的热忱。然而，我发现一些人实际上缺乏必要的数学直觉和知识框架去得到有用的结果。这便是我决定写这篇博文的主要原因。最近涌现出了很多易于使用的机器学习和深度学习的软件包，例如 scikit-learn, Weka, Tensorflow 等等。机器学习理论是统计学、概率学、计算机科学以及算法的交叉领域，是通过从数据中的迭代学习去发现能够被用来构建智能应用的隐藏知识。尽管机器学习和深度学习有着无限可能，然而为了更好地掌握算法的内部工作机理和得到较好的结果，对大多数这些技术有一个透彻的数学理解是必要的。

逻辑回归和神经网络的代价函数的计算方法

为什么要重视数学？

机器学习中的数学是重要的，有很多原因，下面我将强调其中的一些：

1. 选择正确的算法，包括考虑到精度、训练时间、模型复杂度、参数的数量和特征数量。

2. 选择参数的设置和验证策略。

3. 通过理解偏差和方差之间的 tradeoff 来识别欠拟合与过拟合。

4. 估计正确的置信区间和不确定度。

你需要什么水平的数学？

当你尝试着去理解一个像机器学习（ML）一样的交叉学科的时候，主要问题是理解这些技术所需要的数学知识的量以及必要的水平。这个问题的答案是多维的，也会因个人的水平和兴趣而不同。关于机器学习的数学公式和理论进步正在研究之中，而且一些研究者正在研究更加先进的技术。下面我会说明我所认为的要成为一个机器学习科学家/工程师所需要的最低的数学水平以及每个数学概念的重要性。

1. 线性代数：我的一个同事 Skyler Speakman 最近说过，「线性代数是 21 世纪的数学」，我完全赞同他的说法。在机器学习领域，线性代数无处不在。主成分分析（PCA）、奇异值分解（SVD）、矩阵的特征分解、LU 分解、QR 分解、对称矩阵、正交化和正交归一化、矩阵运算、投影、特征值和特征向量、向量空间和范数（Norms），这些都是理解机器学习中所使用的优化方法所需要的。
令人惊奇的是现在有很多关于线性代数的在线资源。我一直说，由于大量的资源在互联网是可以获取的，因而传统的教室正在消失。我最喜欢的线性代数课程是由 MIT Courseware 提供的（Gilbert Strang 教授的讲授的课程）：http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/

2. 概率论和统计学：机器学习和统计学并不是迥然不同的领域。事实上，最近就有人将机器学习定义为「在机器上做统计」。机器学习需要的一些概率和统计理论分别是：组合、概率规则和公理、贝叶斯定理、随机变量、方差和期望、条件和联合分布、标准分布（伯努利、二项式、多项式、均匀和高斯）、时刻生成函数（Moment Generating Functions）、最大似然估计（MLE）、先验和后验、最大后验估计（MAP）和抽样方法。

3. 多元微积分：一些必要的主题包括微分和积分、偏微分、向量值函数、方向梯度、海森、雅可比、拉普拉斯、拉格朗日分布。

4. 算法和复杂优化：这对理解我们的机器学习算法的计算效率和可扩展性以及利用我们的数据集中稀疏性很重要。需要的知识有数据结构（二叉树、散列、堆、栈等）、动态规划、随机和子线性算法、图论、梯度/随机下降和原始对偶方法。

5. 其他：这包括以上四个主要领域没有涵盖的数学主题。它们是实数和复数分析（集合和序列、拓扑学、度量空间、单值连续函数、极限）、信息论（熵和信息增益）、函数空间和流形学习。

一些用于学习机器学习所需的数学主题的 MOOC 和材料是（链接经过压缩）：

    可汗学院的线性代数（http://suo.im/fgMNX）、概率与统计（http://suo.im/CqwY9）、多元微积分（http://suo.im/xh6Zn）和优化（http://suo.im/1o2Axs）

    布朗大学 Philip Klein 的「编程矩阵：计算机科学应用中的线性代数（Coding the Matrix: Linear Algebra through Computer Science Applications）」：http://codingthematrix.com

    得克萨斯大学的 Robert van de Geijn 在 edX 上的 Linear Algebra – Foundations to Frontiers：http://suo.im/hKRnW

    戴维森学院 Tim Chartier 的新课程 Applications of Linear Algebra；第一部分：http://suo.im/48Vary，第二部分：http://suo.im/3Xm3Lh

    Joseph Blitzstein 的 Harvard Stat 110 lectures：http://suo.im/2vhVmb

    Larry Wasserman 的书《All of statistics: A Concise Course in Statistical Inference》，下载：http://suo.im/v9u7k

    斯坦福大学的 Boyd 和 Vandenberghe 的关于凸优化的课程：http://suo.im/2wdQnf

    Udacity 的 Introduction to Statistics 课程：http://suo.im/1enl1c

    吴恩达授课的 Coursera/斯坦福大学的机器学习课程：http://suo.im/1eCvp9

这篇博文的主要目的是给出一些善意的关于数学在机器学中的重要性的建议，一些一些必需的数学主题以及掌握这些主题的一些有用的资源。然而，一些机器学习的痴迷者是数学新手，可能会发现这篇博客令人伤心（认真地说，我不是故意的）。
对于初学者而言，你并不需要很多的数学知识就能够开始机器学习的研究。基本的吸纳觉条件是这篇博文所描述的数据分析，你可以在掌握更多的技术和算法的过程中学习数学。

1， 线性代数（或叫高等代数）：必需，所有的算法最后都会向量化表示，线性代数不熟的话，算法都看不懂啊

2，微积分：这个是所有高等数学的基础，不细说了

3，统计：这里包括统计理论基础，和应用统计（主要就是线性模型）。很多机器学习内容的前身就是统计啊。

3.5， 凸优化： 经 @徐文浩 补充，原因跟6相似

前三个感觉是想要学好机器学习所必需的，后面的虽然不必需，但是适当了解之后，帮助也很大：

4,概率论：基础概率论就够了，以测度为基础的高级概率论对机器学习帮助不大

5，数值分析：数值分析的一部分包括了插值，拟合，数值求解各种方程，数值积分，这些小技术虽然没有跟机器学习直接扯上关系，但是可能在你处理复杂问题时的一些小地方起到奇效。数值分析的另一大块就是数值线性代数了，包括怎么矩阵求逆了，矩阵的各种分解了，矩阵特征根奇异值什么了，这里面很多算法都会被机器学习的书法直接使用的。比如SVD就被Principal Component Analysis直接调用了啊。

6，运筹学：运筹就是做优化，说白了就是把问题表示成数学公式和限制条件，然后求最大值或最小值。所以不少机器学习里面先进的优化算法，最先都是在运筹里面出现的

spark hadoop
自学之家，www.alzhi.com 软件技术联盟
过拟合的解决方式
EM 算法推导
稀疏子空间聚类
多元高斯分布
word2vector, 特征提取
基于矩阵分解的LSI和NMF主题模型
看准
回去找找我的java面试书籍
造卡的核心算法设计，造卡调优，JVM调优
ecgs status monitor, 异步servlet 调优，git相关知识，shell 脚本编写过ecgs configure tool，给traffic做throttlecontrol。selflearning thresthreshold
,通过增加ecgs instance数量 优化failover机制，DB中的数据自动过期.enhance thread pool
飞雪信息
全贝叶斯预测
Softmax回归
神经网络
卷积，RNN，
线性判别分析LDA，Fisher判别准则
skip list，跳跃表
永久代
梯度下降具体怎么操作	
各个分布的期望和方差
指数分布具有无记忆性
二元高斯分布
怎么沿着梯度下降
p(sita | x),  其实 分布模型已经确定，但是分布中的参数，也就是概率还没有确定。比如一个模型服从二项分布，但是参数p，也就是概率p还没有确定
协方差是两个随机变量具有相同方向变化趋势的度量
二维正态随机变量
coredump
CNN ,RCNN,fasterRCNN
偏度，峰度
除以n-1是无偏的
矩估计没有对样本的分布做任何假设
把似然概率看成是sita的函数，就是关于sita似然函数，求似然函数取最大值的时候，参数的取值就是似然估计的结果
参数sita对于不同的分布可能是一个数（泊松分布），也可能是一个向量（高斯分布）。 参数估计的时候，都是假定分布是什么分布的前提下进行的
奇异值分解
p(D)跟sita是没关系的，是个常数。 例如，不管调不调准星，这枪打中的概率就是p(D),p(D)与sita没有关系（或者从全概率公式角度看成是对于所有sita可能值的概率加和，这个和的结果是不含sita的，仍然与sita没有关系，是个归一化因子）
基即为特征
在数值上，似然函数的值， p(x|sita) 与 给定sita后x的后验概率 是相等的，都是 p(x|sita)，不过还是不要这么说，就用贝叶斯公司找那个成正比就行了
正交阵和对称阵都是方阵（正交阵的定义是方阵，其实不是方阵也可以有类似的定义，A是m*n的矩阵，如果A*A转置=I，也可以说是正交阵）
方阵对角线元素和=特征值的加和， 特征值的乘积=|A|
特征值是可以为0的，特征向量是不能为0的。lamadan次幂是An次幂的特征值。lamada逆是A逆的特征值
k近邻矩阵 
实对称阵的特征向量可以取实向量
实对称阵不同特征值的特征向量不光线性无关，而且正交。
对称阵可以用特征值和特征向量对其进行对角化，即合同变换（变换为特征值对角阵，用于线性变换的矩阵为特征向量组成的矩阵），在合同变化中， AU=Ulamda，并不是AU=lamdaU. A 是矩阵，U是特征向量（列向量）组成的矩阵，lamda
是特征值组成的对角阵
普通矩阵也有合同变换的定义。只不过用于变换的矩阵不一定是特征向量组成的矩阵，变换成的矩阵也不一定是特征值组成的对角阵。只有对称阵有那么强的结论
JVM gc分析和命令是用，看年轻代，永久带什么的
jmap
主成分分析
数据的白化(漂白)
去均值ICA。只做去均值（中心化），不做白化效果就很好了，做了白化反而会造成处理的过分了
CNN，卷积神经网络
正定阵就是正数在n维空间的推广
A的转置乘以A是半正定的对称阵。 A是任意m*n阶矩阵
向量求偏导
QR分解可以求特征值。 Q正交，R非奇异上三角
点乘就是二范式的平方
判定定理1：对称阵A为正定的充分必要条件是：A的特征值全为正.
判定定理2：对称阵A为正定的充分必要条件是：A的各阶顺序主子式都为正
判定定理3：任意阵A为正定的充分必要条件是：A合同于单位阵.
正定矩阵的性质：
1.正定矩阵一定是非奇异的.非奇异矩阵的定义：若n阶矩阵A的行列式不为零,即 |A|≠0.
2.正定矩阵的任一主子矩阵也是正定矩阵.

正定矩阵的定义是，所有特征值都为正。正定矩阵一定和单位阵合同。两个矩阵合同，则两个矩阵的特征值的正负个数一样。
几乎所有不等式都可以看成是jensen不等式和某一个凸函数结合得到的
KL散度
Jensen不等式
关于拉格朗日对偶函数是凹函数的解释。https://blog.csdn.net/u014540876/article/details/79153913
max(minf(x,y)) <= min(maxf(x,y))
 y   x             x    y
 
KKT 条件下，上式才能去等号
最小二乘法
不同的损失函数对应着不同的机器学习模型
SVM损失就是hinge损失
nunmpy.ndarray  a=np.array(L)
ctrl+shift+/ 批量注释
奇异值分解SVD可以看做是对称阵（还是方阵？应该是方阵）在任意矩阵上的推广。虽然只有方阵才有特征值和特征向量，但是SVD可以看做是个推广。SVD用于隐特征推荐
线性变换的特征向量是指在变换下方向不变，或者简单地乘以一个缩放因子的非零向量。
PCA和SVD的应用场景不同
指定不同的卷积核可以得到不同的数据。卷积（滑动相乘相加）
多做几次小的卷积比做一次大的卷积在结果上要好。标配是3*3的卷积核，补齐（peding？）的时候是补的1. 步长 stride一般是1. 在N*N的矩阵上，每四个元素取一个最大值，则变成了N/2*N/2,这个叫pooling（就是下采样）.
残差网络？
集成上往往用的是像决策树这种弱分类器，而不适用 LR或者SVM这种强分类器。。经典的随机森林就是决策树形成的。决策树不加剪枝的话是一定会过拟合的。
范德蒙德行列式可以用来证明插值法是可行的
在实践中，对于分类问题，首选的方法就是Logistic R，它有一个竞争者，就是SVM
（ka分布，t分布不一定要会，看看面试题就知道了）
视频都是在特征已知的情况下进行的。那特征怎么提取
超参数的确定可以用交叉验证。gridsearchcv
在实践中，不同的模型可能会有一点诧异，但是影响结果最大的是特征的选取。特征选的好，业务熟悉，特征抽取的好，就能有好的结果。
线性回归的结果不一定是直线，也可以是曲线，用2,3或者高次（会过拟合，龙格现象：高次下，系数太大了）曲线去做拟合
IID 独立同分布
最小二乘法
在线性回归中，每个 y值都是独立同分布的。服从均值为 Hsita(xi)，方差为sigma的高斯分布。 对于式子， yi = hsita(xi) + epcinli, 这其实是m个式子
每个式子都代表 yi这个随机变量的分布情况。 并不是  各个 yi 之间有什么关系，它们都是独立同分布的，假定它们的方差是相同的。
线性回归中(做一个Xthea趋近于y的过程，当然X可以是多次方的，线性指的是对于thea来说是线性的)，残差假定服从正态分布是一个理想化条件。对于线性回归模型,当因变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计.
线性回归是广义线性模型，它的函数指数簇就是高斯分布
K均值假定的是混合高斯模型
java内存管理，时空复杂度
thea转置乘以A乘以thea对thea球偏导，如果A是对称阵，则结果为2Athea
正定矩阵一定可逆。这不废话吗
正则化：l2正则就是l2norm，是在损失函数上加上系数的平方和作为正则项，也就是Ridge回归。l1 正则是在损失函数上加上系数的绝对值的加和作为正则项，也就是LASSO。
l2正则往往会学的指标比较好，但是l1正则往往会是某些thea参数为0
等高线是个什么鬼
交叉验证， 比如十折交叉验证。gridsearchCV
普通矩阵没有逆，只有方阵有逆。但是可以给普通矩阵定义一个伪逆（广义逆），广义逆可以用SVD解释或者他俩结论正好一样。
特征值分解和SVD的关系
对于梯度下降来说，假如说是y=x 这种二维的。 负梯度方向其实就是x轴的正向或者是负向。加如是z=f(x,y)这种三维的。 那负梯度方向就是 在 xoy 这个平面上的一个方向，当 x，y沿着这个方向走一点的时候，z有一个变化。并不是说
直接在z的图形上向下走了一点，而是在xoy平面上 x，y 走了一点，而函数值z就像局部极小值走了一点，看着像是z 直接走了一点似的
对线性回归来说，因为J是一个凸的二次函数，所以用梯度下降算法找到的局部极小值就是全局最小值。对于其他模型来说，梯度下降算法不一定能找到全局最小值，它能找到的是局部极小值，但一般这个局部极小值就够用了，就用这个局部极小值了
批量梯度下降和随机梯度下降（SGD），实践证明，随机梯度下降是可行的，并且是最常用的（这样可能噪声也被计算了一次，而且不一定会下降，但是总趋势是下降的，而且噪声不一定是坏事，可能会帮助跳出局部极小值而找到全局最小值，而且随机
梯度下降速度快，因为拿到一个样本就下降了一次）。RNN，CNN用的是随机梯度下降。还有一个好处是并不不要把所有样本都拿到再下降，拿到一个就可以下降，样本可能是可以流失的，有可能不能在手上一直拿着（在线学习）。
在线学习就用随机梯度下降。
折中一下，不是拿到所有样本再下降，也不是拿到一个就下降，可以攒够多少多少个样本再下降一次： mini-batch 梯度下降，这个是实践中用的更多的。
线性回归，关于参数是线性的，关于x可以不是线性的，所以那个分界面不一定是线性的。如果加了正则项之后不收敛，那就说明参数给的不合适，跟算法本身没有关系。学习率参数给的小，像下走的慢，给的大，有可能不收敛，震荡
l1正则可以用l2去近似或者把l1正则项给近似喽
自己写一下梯度下降算法的线性回归
SVM也是线性分类器
防止过拟合，可以增大样本数目。 在线性回归中，可以加入 l1或者l2正则(可以防止thea参数过大，也就是防止龙格现象。而产生过拟合。)
R平方=1- RSS/TSS. R 平方越大，拟合效果越好
TSS和ESS和RSS的关系。ESS又称为回归平方和SSR
高斯核函数，多项式核函数
logistics回归，只要在实践中去做分类，工业界，首选的分类算法就是 logistics 回归。logistics的多分类版本就是softmax回归。 sklearn还是叫logistics函数，通过给他的参数个数不同，决定是二分类还是softmax多分类
sigmoid函数。 g(x)'=(g(x)*(1-g(x))
线性回归有解析解，logistics回归是没有解析解的。大多数都没有解析解，线性回归太简单了，赶巧了有解析解。logistics只能用梯度下降来算，线性回归也推荐用梯度下降来算，其实logistics是沿着对数似然函数的梯度上升的，
只不过也可以把它叫做梯度下降
logistic参数学习的规则和线性回归参数学习的学习规则是一样的。
一个事件发生的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。logistics回归的对数几率是theax，是线性的。所以logistics回归可以看成是广义的线性回归(它的对数是线性的)
logistics的目标函数是最大似然来的,是用最大似然推导的。也可以给它一个损失函数，loss()=-l(thea)  NLL，负对数似然。
数据生维：选取特征。有时候线性的没法很好的把样本给分开，那就用样本数据的高维以及高次，比如 本来是 1，x1,x2 三个维度，分割面是直线，我们可以这样选取特征，1，x1,x2,x1平方，x1*x2,x2平方，相当于升维，这时候分割面就是
曲面了
凡是符合指数族分布的随机变量，都可以用GLM回归分析。 （Generalized Linear Models）
logistics回归和softmax回归是实践中解决分类问题最重要方法。特征选择很重要，除了人工选择，还可以用其他机器学习方法，如随机森林，PCA，LDA等用来构造出特征。
PCA和SVD的区别。
梯度下降算法是参数优化的重要手段，尤其SGD，适用于在线学习，跳出局部极小值。（怎样用梯度下降球矩阵的逆）
对于分类器而言，给定不同的阈值，就会有不同指标 TP,FP,TN,FN,。AUC，ROC，
奥卡姆剃刀。
softmax 梯度公式
mse，rmse
np.split()怎么用
lasso也可以用于 lr。 lr默认是 logistics回归
决策树，若干颗树形成随机森林。决策树可以做分类和回归（classrification and regresion tree, CART）
树的深度越深，越容易过拟合
建立一颗决策树的关键是，知道一个节点怎么分，并且可以递归的调用分法来对子节点同样进行划分，这样决策树就建立好了。借助一点外力的算法生成多颗树，随机森林就建好了
熵表示一个事件发生的不确定性，也就是信息量的大小
熵了联合熵的定义式子。。  以2为底时，熵的单位是bit，以e为底时，熵的单位是nat
条件熵
KL散度可以引出logistics回头。   KL(p||q)  ，p是已有样本的分布，q是要去就算的样本的分布，要想让他们俩尽量接近。
决策树不一定是二叉树。
在根节点，熵是有一个值的，在叶子节点 熵为0，只要熵为0，就意味着分对了。 由此推出，划分方式为，那种划分使熵的下降最快，就选这种方式。这样一个生成过程是熵逐渐降低的方法，也是贪心法，greedy。这个不一定是最优的，
只能保证局部最优，但不一定是全局最优解，贪心找的就是局部最优。
线性回归为什么梯度下降算法找到的是全局最优解
ID3，interactive dichotomister，迭代二叉树
互信息。
经验熵，经验条件熵。信息增益g(D,A) = H(D) - H(D|A), 即为训练数据集的D和特征A的互信息。哪个特征和我D的互信息最大，就选哪个特征进行分割。
step：
1. 计算H(D)
2. 遍历所有特征，对于特征A，计算H(D|A)
3. 计算g(D,A)=H(D) - H(D|A)
4. 选择g最大的特征作为当前的分裂特征。
对于unbalance的数据，任何机器学习的模型都没有好的办法，所以现在的办法就是队数据进行一些预处理，决策树和随机森林是方便做这些事情的。
c4.5: 不用信息增益作为当前划分的度量，而是用信息增益率作为当前划分的度量，信息增益率就是D和A的互信息除以A的熵。
gini系数， CART树
特征A也是有概率和不确定度的（A也可以看做是随机变量，比如变量的值取 sunny还是rainy还是windy等等都有各自的概率）。 A 由n的类别组成，则P(Ai) = 第i个类别的样本个数 /  总样本个数。特征A的熵也可以按熵的定义算出来。
ID3, c4.5, CART树算法，都是贪心算法。都可以看成是，一个属性的信息增益，或率，或jini系数越大则表明属性对样本的熵的减少能力越强，这个属性使得数据由不确定性变成确定性的能力越强。
叶子才是导致最终分类结果的依据。 纯节点，均节点, 分的越细，损失越小，当然这只是在训练数据集上的损失越小。
决策树的评价函数c(T) 即为损失函数。真要c(T)为0了，那肯定就过拟合了
通过二叉树是可以做多分类的，通过N叉树是可以做二分类的。 几叉和最终几个类别的个数是两个概念
防止决策树的过拟合可以用两种方法，随机森林和预剪枝和后剪枝
预剪枝：边生成树的过程中边去剪枝，比如深度最多不能超过多少层，比如叶子节点中的样本个数小于多少个就不能再分，比如叶子节点的熵值小于多少就不能再分
后剪枝：先生成树，再去剪枝。对于内部节点，肯定可以把它的 alpha值都算出来。把最小的剪枝掉，再计算，再砍，再计算，再砍，就得到了若干颗树，用测试数据看这些树哪个损失最小，就选那颗树。alpha称为节点r的剪枝系数
随机森林在bagging的基础上做了修改。bagging：通过bootstrap方法（大概能覆盖样本的%63）得到很多决策树，用这些决策树进行投票得到最后的分类结果
随机森林，在选择作为分裂的特征的时候，不是遍历所有特征选最优的，而是随机选出一些特征，从中选最优的。在实现的时候，可以这样，给一个随机数，遍历所有特征
的时候，如果这个随机数<一个值，比如0.8，这个特征就加入到备选中，假如>0.8,就舍弃掉。这就相当于选了80%的特征进入到备选特征中
dropout 有点像随机森林。随机森立的竞争者是SVM，现在做卷积网络 往往为了过拟合 就有点累类似随机森林。
可以把随机森林的思路用到别的分类模型上。比如在线性回归中，用多项式去拟合曲线时，可以从样本中多次采样，生成多条曲线，做个平均，得到一条曲线。不一定从N个样本中采样出N个，可以采出一部分，但是得交叉验证。
关于投票，大多数是少数服从多数。但是对于 像电影投票那种要加上一定的权值。
关于样本不均衡的做法。可以不只关注与准确率指标，比如关注AUC指标，能好一些。还有其他方法：
假如样本数量A的比B的多很多，且严重不平衡，则可以采用以下一些方法。
1. A欠采样。欠采样可以随机欠采样，每次采样出一部分去跟B做个分裂期，再采出一分区去与B做个分类器，这些个分类器（可以是树）组成个森林，去做分类。也可以对A进行无监督的聚类，由A的簇去和B做分类。
2. 跟欠采样相对的就是 重采样（过采样），可以避免欠采样造成的信息的损失。但是实践发现， 欠采样比过采样的效果要好。 欠采样的样本少，所以速度快，过采样反而会造成样本数量的增大。
3. B类数据合成，随机插值得到新样本，SMOTE。这样就把小类的数目就增多了
4. 代价敏感学习。降低A类权值，提高B类权值。实际做的时候，像 100， 10 这样的数据，就给B类给个权值2就差不多了。
前三类是基于样本做事情，算法没变，第4是基于算法做事情。 算法，样本 两方面的考虑。
随机森林有样本的随机和特征的随机，所以同样的随机森林不一定能重现。
随机森林是可以做 树的防止过拟合的。（只要是足够多的树就可以防止单颗树的过拟合）。但是在实际应用中，还是会防止单颗树的高度太高，一棵树的叶节点不要包含过少的样本值
对于随机深林而言，调参并不难，因为深度不同 分类结果都差不多。（深度学习的调参是很难的）
写代码的时候，要学会使用 pipeline
决策树也可以做回归。在决策树作回归的时候，不再是用熵下降速度最快为准则来劈开节点，而是用均方误差最小准则（mse）来劈开节点。具体是怎么操作的? 
局部加权 线性回归是个啥
决策树是可以有多输出的，不光是决策树，其他模型也行一样，y值可能是有两个值。想想决策树单位圆的例子。
entroy
在非纯的叶节点，通过少数服从多数的原则来决定样本属于哪一个类别。
ID3的缺点，倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树；另外输入变量必须是分类变量（连续变量必须离散化）；最后无法处理空值。
C4.5选择了信息增益率替代信息增益。
CART以基尼系数替代熵；最小化不纯度而不是最大化信息增益。
而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），
在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。
随机森林在bagging的基础上更进一步：
1.  样本的随机：从样本集中用Bootstrap随机选取n个样本
2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）
3.  重复以上两步m次，即建立了m棵CART决策树
4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、 加权多数）

提升，可以用于分类和 回归中，它每一步都产生一个弱分类器（如决策树，一般用CART树），并加权累加到总模型当中。如果每一步的弱预测模型生成都是依据损失函数（预先给定一个损失函数）的梯度方向，则称之为梯度提升。
提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。（提升和bagging达到的目标貌似是相同的，但是方法不同）
线性回归做的损失函数就是 均方误差。 假如用均方误差作为损失函数，其实是我们认为 y 是服从高斯分布的。 选择哪种函数作为损失函数，是根据先验经验来的，我们认为这个分布可能服从什么分布，就选哪种损失函数
取损失函数为平方和（高斯分布）的依据，图形。 取损失函数为绝对值和（拉布拉斯分布）的依据，图形。 （都是指数函数的图形。）
长尾分布是个啥，指数族分布。
为什么会那么喜欢平方和损失：样本的均值是可以让平方和损失最小的定值。所以，反推回去，在提升中，就用用样本的均值作为第一次估计。（如果把所有的训练样本值都预测为一个定值，那当这个定值是所有训练样本的
均值/中位数/众数等的时候，L最小。但这是把所有值都预测为了一个定值的情况，在真正建立完成了的分类器中，当然对于每一个测试数据，不会都预测为一个定值的，都要根据这个测试数据的特征来进行预测的，最终得到的L肯定会比
全用一个定值来预测要好。但是对于只用一个定值来预测的时候，用均值/中位数/众数的效果是最好的）
在用平方和损失时，样本均值是使得损失函数最小的值。在用绝对值和损失时，中位数是其绝对最小最优解。   
l0,l1,l2正则。L1正则，认为先验分布服从拉普拉斯分布，l2正则认为服从正态分布，l0正则认为服从一个那啥分布（对于这个分布，u取样本的众数可以使得损失最小）
梯度提升方法寻找最优解F(x)，使得损失函数在训练集上的期望最小（其实线性回归中也是这个目标，其他机器学习模型的目标是不是这个?）.方位为：
1. 给定常函数 F0(x)，这个常函数在前面说了，根据损失函数的形式不同，可以能是样本均值，样本中位数或者众数或者别的什么东西。
2. 通过贪心的思路扩展到Fm(x).
梯度下降在每次选择最优基函数f时仍然困难，是用梯度下降的方法近似计算。 
boosting与bagging的区别

    bagging通过有放回的抽取得到了S个数据集，而boosting用的始终是原数据集，但是样本的权重会发生改变。
    boosting对分类器的训练是串行的，每个新分类器的训练都会受到上一个分类器分类结果的影响。
    bagging里面各个分类器的权重是相等的，但是boosting不是，每个分类器的权重代表的是其对应分类器在上一轮分类中的成功度。

AdaBoost是boosting方法中最流行的版本
AdaBoost算法可以看做是采用指数损失函数的提升方法，其每个基函数的学习算法为前向分步算法；

   AdaBoost的训练误差是以指数速率下降的；

   AdaBoost算法不需要事先知道下界γ，具有自适应性(Adaptive)，它能自适应弱分类器的训练误差率。
logistics回归的损失函数
残差网络
梯度提升的典型基函数即决策树，尤其是CART树
boost是串行的。
GBDT相当于不是对于一棵树整个权值的调整，而是对于叶子权值的调整。
对训练集拟合过高会降低模型的泛化能力，需要使用正则化技术来降低过拟合。通过衰减因子和降采样防止过拟合。
要么告诉我损失函数长什么样子，logistics损失，误差平方和损失，svm损失，或者告诉我一阶导和二阶导是什么。
决策树也是有正则项的。正则项怎么加？可能用树的深度？节点个数？每个叶子结点含有样本数量？
树最重要的是叶子。是用决策树对样本做分类（回归），是从根节点到叶节点的细化过程；落在相同叶子节点的样本的预测值是相同的。
一个决策树的核心即 “树结构”和“叶权值”。
决策树的复杂度可考虑叶节点数和叶权值，所以定义正则项的时候可以这样来定义下，并不是唯一的。叶节点总数和叶权值平方和的加权。
GBDT使用的一阶导的信息，也可以是用二阶导的信息。
手写快速排序、手写二分查找
在梯度提升决策树中，如何构造新决策树：即，对于当前节点，应该如何进行子树划分。借鉴ID3,C4.5,CART树的做法，使用贪心法.
1.对于某可行划分，计算划分后的J(f).
2.对于所有可行划分，选择J(f)降低最小的分割点。 当然分割的时候，不一定非得分割成二叉树。
在你所知的算法中，哪个抗噪能力最强？哪个对采样不敏感？
XGboost，GBDT的24页，讲的不对。  J是越小越好，所以-J是越大越好。 所以增益就是分割后的 -j减去 分割前的 -j，这个差值越大越好，而且分割后的T=2，分割前的T=1.
相对于传统的GBDT，XGBoost使用了二阶信息，可以更快的在训练集上收敛。XGboost也可以认为是随机森林的推广，只不过树不是随机去做的，而是给定了权值的，每一棵树的叶节点都给了w值的。
XGboost的实现中使用了并行/多核计算。原生语言是C。
如果对样本也给予权值，就是adaboost
在adaboost中，权值既给分类器用，又给样本用。
adboost怎么做多分类
adaboost在每次做基本分类器的时候，是用的什么分类方法？
不管是随机森林，xgboost，adaboost还是gbdt，多做几棵树然后加权的话，均值是不变的，还在中心处，但是方差就会变小，这个结论很好。方差太大就会过拟合，
所以对于随机森林，只要树足够多，方差就能够下去的。
bagging能够减少训练方差，对于不剪枝的决策树、神经网络等学习器有良好的集成效果。   |想想图
boosting减少偏差，能够基于泛化能力较弱的学习器构造强学习器。                      |想想图
GBDT和随机森林的区别是什么。
在XGboost中，在做某一刻树的某一个分割点的时候，选择增益最大的划分，继续同样的操作，直到满足某阈值或者得到纯节点。
没有项目的话，可以在kaggle比赛中锻炼一下。
传统的GBDT只是用到了一阶导的信息，二GBDT用到了二阶导的信息。在树的建立时，是没法并行的，得一颗一颗的建，这里肯定是串行的。在遍历某一个节点的可行分割点的时候，可以并行。
多分类的时候，不要简单的吧类别或者特征表示为 0,1,2 这些数值。因为这些数值一乘一加，意义就变了。这些数值应该是不可乘不可加的。加入一个特征分为三个值，不要把这三个值记为0,1,2.而是把这个特征表示为一个向量（1,0，0），
（0，0，1）。（0，1，0）。one-hot编码方式
有时候，用强分类器，即使做了bagging，效果并不好。
在存储特征的时候，用onehot编码方式是对的，但是特征太多的时候（稀疏数据），用onehot存储的值太多了，可以键值对只存储那些有值的特征，比如，12:1,34:1.意思就是，第12个特征和第34个特征是1，其他都是0.SVM和LDA等都是这么存储的。
在xgboost中，往往会给每一棵树一个衰减因子，防止过拟合。
xgboost是重点。
在xgboost中需要传进去一个参数是 'objective':'binary:logistics'   或者 'objective':'binary:logitraw'。 这个是干嘛的。这样应该就是J(f(t))?  在做其中一颗决策树的时候，我们必须知道它一阶导和二阶导的信息，给定objective
形式就相当于给了一阶导和二阶导的信息。 那其实给定的这个logsitics就是给定的目标函数 J(f(t))
在xgboost中，可以自定义一个损失函数传给xgboost。
data=pd.read_csv(file_name).  这是最方便的读csv的做法。
可以认为xgboost是对gbdt的改进，因为后者只用了一阶导信息，二xgboost用到了二阶导信息。
notify,notifyall
SVM的优势是泛化效果好，分类的效果也好（如果不考虑训练时间只考虑使用的时候）。 LR的好处是能给出一个后验概率
其他分类器中也有核函数的概念。
SVM分为：线性可分（硬间隔）SVM，线性（软间隔）SVM和非线性SVM。
交叉验证来确定最佳参数。
SVM中，只要找到支撑向量，就能做出分类器来，反之，如果有了这个分类器，那哪些样本点是支撑向量也就知道了。（因为样本点都是n维的，所以一个样本点可以看做是一个支撑向量）
SVM 可以做多分类，只是用二分类来推导的。在讲SVM的时候，超平面的表示 并不是 y=w转置*x +b.  而是 w转置*x + b = 0.  w和x都是向量。 w是这个直线的法线方向
取什么样的特征映射其实是个超参数。把低维特征映射到高维。polynomialFeatures()就是做的多项式特征提取。就是对应的这个特征映射。用特征函数  fai 去映射
求解分离超平面问题可以等价为求解相应的凸二次规划问题。
线性SVM只要加一个核函数就可以变成非线性SVM（分割面是曲线，不是直线。）
最小二乘法，拉格朗日乘子法。
满足KKT条件， 最大最小才等于最小最大。KKT条件需要复习一下,需要再看一下凸优化那的视频
一个数乘以一个向量再对这个向量求导，和一个数乘以这个向量的转置再对这个向量求导的 结果都是一样的，都是这个数。因为一个数的转置还是这个数本身。
不一定分类完全正确的超平面就是最好的：
分类完全正确的可能把离群点也算进去了，这样可能导致margin特别小，而有个别离群点可以不去考虑的时候，margin就变大了，这样泛化能力就变强了，过拟合可能就降低了。
有时候样本数据本身就线性不可分。
线性SVM在目标函数上要加上一个松弛因子。
SVM等比例缩放那我还是不明白。先就这样理解了。
几何间隔大小是不受参数大小缩放影响的，那么我们可以通过调节参数w,b使得函数间隔为1，即γ^=1，而此时的几何间隔并不会发生变化，也就是直线不会变化。
5. SVM的优缺点

优点：
SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。
缺点：
SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到O(n2)
的复杂度，这对大规模的数据是吃不消的。


对于存在分类间隔的两类样本点，我们一定可以找到一些决策面，使其对于所有的样本点均满足下面的条件: 也就是函数间隔>=1.
SVM 可以看做，是给定松弛因子作为我们的目标函数，只不过加了一个l2的正则项：lamda||w||平方。 比如人要问你从损失函数的角度说说，SVM跟岭回归的关系，那就是这个。
正负支撑向量，样本点到分割面的距离都是d，这是一个限制条件，在这个限制条件下，才去找的最优的w,b使得 间隔最大。去看那个0基础的SVM那个教程吧。
对于函数距离的缩放，并没有影响几何距离，所以就另函数距离为1.
几何距离和w,b 并不是一一映射的。 当直线定下来后，几何距离就定了，但是这时候 w,b并不是确定的，能确定的只是w 和b的比值，可以是2w,2b, 可以使3w,3b. 但是这个时候函数间隔就会有变化。所以函数间隔对于这个问题的优化
是没有影响的。在寻找几何距离的最大值的过程中，当给出一个超平面，无论到它的几何距离是多少，都可以通过调节w和b的值(但是保证比值不变)，使得||w||的值为 几何距离分之一，这时候函数距离就为1，所以所函数距离对于我找到
这个超平面没有影响。

其实换个角度思考下可能更简单理解： 超平面 wx + b =0; 最小函数间隔(Xm点取得)：r^ = y * (wXm+b) 约束条件：y * (wx+b) = ri^ > r^ 目标函数：max(r^/||w||) 超平面改写： k(wx+b) = 0 --> (kwx + kb) =0 最小函数间隔(依然Xm点取得)：r^ = y * (kwXm+kb)--->原来的k倍 约束条件：y * (kwxi+kb) = ri^ > r^ 
目标函数：max(r^/ ||kw||) --> max(r^ /k||w||) 把r^代进去，约束条件，目标函数均没有变化，且是同一个超平面，但是函数间隔却变了k倍，说明任意伸缩函数间隔，对目标问题求解没有影响。 关键是要理解好函数间隔和几何间隔 打个比喻： 给定一个超平面A， 约束条件里，是用函数间隔来量度样本点距A的相对远近(就
像我拿同一个放大镜看几条绳子的长短，尽管得到的不是真实长度，但是对所有长度放大倍数一样(放大倍数k任意大小)，所以可以比较)。 记这个相对最小距离为r^。 而真实的远近是需要用几何间隔来量度的， 目标求解中， r^2/||w|| 就已经除去k的影响了。(把相对大小转换为绝对大小)，把 r^缩放成1，就是我们要的
目标函数了

高斯核函数可以看做是把数据x映射到了无穷维所得到的。假如两类样本点在某一维度上是不可分的，那我们可以用高斯核函数把样本点映射到高维度上，进行分割。就像大米和米麸在一个平面上是不可的，但是我们给倒水之后，在三维空间中
就是可以分开的。从水面上看，其实大米和米麸都在一个平面上，有那么一个投影的关系在。高斯核又能映射到无穷维，又能保证低维的权值大，高维的权值小（不那么在意这些高维），结论很好

当数据量是中小量的时候，应该用SVM，如果数据量大的话，可以用logistics回归。
SVM调参很重要，需要指定一个C，用核函数的话还需要制定一个sigma。 C越大，margin越窄，过拟合的可能性越高。C越小，margin越宽，过拟合的可能性越低，但是不能直接给一个比较小的C，因为可能会欠拟合，也就是精度降低了。
RBF，高斯核函数，也叫做径向基函数。
对于unbalance的数据，可以给特别少的那些样本一个比较大的权值，但是不用那么大就可以。 具体权值取多少合适是需要交叉验证的。
把预测值和实际值 那个表格画出来， 各种率都能知道了。
预测值     P              N 
实际值
   正      TP             FN
   负      FP             TN
   
 recall就是查全率（在所有真正有病的人当中，我们认为他有病的概率。）。   precision就是查准率（在所有我们认为有病的人当中，真正有病的人的概率）。但是两个指标是不方便用的。我们会给出一个他们加权的指标。
 F1 score,或者叫 F1 measure.给出的是 他俩相同的地位。 还有F beta score,beta从0到无穷大。beta趋近于无穷大的时候，是recall，beta趋近于0的时候，是presision （这些都是针对二分类的，如果不是二分类，只能是给变成1 vs rest） 
 1 vs rest什么意思
 
 SVR，SMO，还是应该看看 july大神还有陈老师的博客。SVM怎么做多分类
 SVM 那推导还有加了惩罚因子（线性SVM）那的推导还是应该再熟练一下。
 从SVM实战那视频中，看样子，支撑向量不光是位于支撑面上的样本值，还有超越了支撑面，进到过渡带里面的样本值。
 人的状态是 紧起来容易，但是紧起来，松下去，再紧起来难。
 SVM中对于不均衡样本，给少的样本一个较大的权值之后是怎么处理的？也可以生成小样本，或者聚类等去处理，没有放之四海皆准的准则，只能case by case.
 当gama值给的非常小的时候，SVM+RBF就相等于KNN。 在SVM+RBF 中，想一下老师用画板画的那个图，想想gama大小对于分类效果的影响。
 如果用k近邻是可行的，那用SVM+高斯核函数一定也是可行的。
 高手都是慢慢磨练出来的，要多写点东西。
 事实上 gamma和C都是应该 交叉验证得到的。 把 x,y 分开，交叉验证来确定gamma 和c
 SVM+高斯核函数 做 SVR是有一定的预测能力的。
 gamma和c需要通过 gridsearchCV来确定。
 用PCA做SVD。
 聚类，是无监督的。根据数据内在相似性来分类。类别内的数据相似度较大而类别间的数据相似度较小。
 内在想法：如果看到一个算法是做降维的，那可能它可以用来做聚类，那可能它就是一种无监督的学习。 有时候，降维和聚类是可以划等号的， 聚类和无监督是可以划等号的。
 度量相似性最简单的是欧氏距离。p=2是欧式距离，p=1是街区距离或者叫曼哈顿距离，p=无穷大是哪个维度相差的距离大，就取哪个距离的差，叫切比雪夫距离。 其实这个p取多少，可以交叉验证搞一把，但是一般用2.
 杰卡德相似系数，余弦相似度，perarson相似系数。相对熵(K-L距离)（这个是关于随机变量的相似性，这个相似性是不对称的），Hellinger距离（当lapha为2时，可以推导出它是对称的。当alpha趋于0的时候，可以退出kL散度）。
 K-L散度取最小的过程就是最大似然估计去最大，两者本质是一样的。也是交叉熵取极值的过程。（要会推导）
 相关系数即将x,y坐标向量各自平移到远点后的夹角余弦。 这即解释了为何文档间求距离是用夹角余弦，因为这一物理量表征了文档去均值化后的随即向量间相关系数。
 K均值和K近邻的区别
 K-mediods聚类（K中值聚类），使用的不是均值，而是中位数。能在一定程度上减少对于样本的敏感性。（异常点，比如1,2,3,4，100 的均值为22，中值为3）
 在K均值算法中，如果初值选的不一样，那聚类结果也可能不一样。也就是K-meas算法是初值敏感的。
 K-means算法的终止条件是，  迭代次数\簇中心变化率\最小平方误差。MSE.
 二分K-means。
 最为常用的选聚类中心的办法。距离做加权来选聚类中心：kmeans++算法。
       首先需要明确的是上述四种算法都属于"硬聚类”算法，即数据集中每一个样本都是被100%确定得分到某一个类别中。与之相对的"软聚类”可以理解为每个样本是以一定的概率被分到某一个类别中。

      先简要阐述下上述四种算法之间的关系，已经了解过经典K-means算法的读者应该会有所体会。没有了解过K-means的读者可以先看下面的经典K-means算法介绍再回来看这部分。

     (1) K-means与K-means++：原始K-means算法最开始随机选取数据集中K个点作为聚类中心，而K-means++按照如下 的思想选取K个聚类中心：假设已经选取了n个初始聚类中心(0<n<K)，则在选取第n+1个聚类中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1)时同样通过随机的方法。可以说这也符合我们的直觉：聚类中心当然是互相离得越远越好。这个改进虽然直观简单，但是却非常得有效。

      (2) K-means与ISODATA：ISODATA的全称是迭代自组织数据分析法。在K-means中，K的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。

      (3) K-means与Kernel K-means：传统K-means采用欧式距离进行样本间的相似度度量，显然并不是所有的数据集都适用于这种度量方式。参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类，就有可能改善聚类效果。
	  
K均值算法在迭代中，用均值去更新聚类中心的做法，本质上就是梯度下降做法。平方误差（如果不是平方而是绝对值，就是k中值算法，取求平方的过程，本质上是认为xi服从均值为uj，方差为某一个数的高斯分布，一共有K个，即GMM）作为目标函数。因为目标函数不止有一个极小值，所以取的初值对于 梯度下降最终能下降到哪停止是有影响的。也就是初值敏感。 
所以K均值算法默认了样本服从 GMM 分布。如果是K中值算法，就是默认样本服从混合拉布拉斯分布。
K均值算法也分为批量梯度下降和随机梯度下降。 BGD，SGD。mini-batch k-means。
K均值算法的算法特点(高斯分布)，决定了它只能把样本聚成类似圆形的区域。
K的选取最直接的办法就是交叉验证。（肘部算法也了解一下。）
K-means有时候去多做几次，找最好的结果。
canopy算法，可以归为聚类算法，但更多的可以试用其做空间索引，其时空复杂度都很出色。这个算法，一个样本属于多个簇。 canopy+kmeans怎么操作？
聚类的衡量指标：均一性，完整性，v-measure，ARI,AMI，轮廓系数  C是类别，K是分类(簇)， 均一性就是一个簇里只有一个类别，即，给定簇了，那类别就定了，定了就是熵为0，H(C|K)=0, 完整性就是一个类别都分到一个簇里，即，给定类别了，那簇就定了，
定了就是熵为0，即H(K|C)=0.  为了让均一性和完整性 从0到1,1最好，0最差，所以做成了这个样子， 1 - H(C|K)/H(c), 1 - H(K|C)/H(K), 然后在 H(c)和H(k)的地方补上定义0，就是这个样子了。
轮廓系数： s(i) = b(i)-a(i)/max{a(i),b(i)}  s接近1，说明样本i聚类合理，接近-1，说明样本更应该分到别的
簇，若接近0，说明样本i在两个簇的边界上。	无监督一直有这个问题：不知道对错。

大数定理：频率近似概率
均一性，完整性，v-measure，ARI,AMI 都是需要知道实际的类别是什么。如果不知道的话，可以用轮廓系数。
AGNES和DIANA算法，一般用凝聚，而不用分裂。 层次聚类注意适时停止，不要走过了。
密度聚类可以克服基于距离的算法只能发现类圆形（凸）的聚类的缺点，可发现任意形状的聚类。且对噪声数据不敏感，而噪声在k均值算法中是致命的。
DBSCAN
概念：核心对象，直接密度可达，密度可达，密度相连，簇：一个基于密度的簇是最大的密度相连对象的集合。噪声：不包含在任何簇中的对象称为噪声。
并查集。密度聚类的时间复杂度和空间复杂度是让人头疼的。
密度最大值聚类，可以识别各种形状的类簇，并且参数很容易确定。
谱聚类，方阵作为线性算子，它的所有特征值的全体统称为方阵的谱。谱半径是所有特征值里最大的。  矩阵A的谱半径是是 A转置乘以A 去求最大特征值。
AP 相似度传递算法。
度量两个样本的相似度可以用高斯相似度。 （RBF）
在谱聚类中，L=D-W. 0是L的一个特征值，全1是它对应的特征向量，不管L长什么样子。谱聚类有三种L矩阵。未正则的拉普拉斯矩阵，和正则的拉普拉斯矩阵，正则的拉普拉斯矩阵包括两个，一个是对称拉普拉斯矩阵：L两边乘以D的二分之一次方（这个需要对矩阵U按行单位化）
还有随机游走拉普拉斯矩阵，L左乘一个D的逆。 没有其他信息的时候，首推随机游走矩阵。。
在实际做的时候，做这个W矩阵的时候，可以是正常做，或者K近邻方式做W（选前K个最大的，后面的都置为0），或者做带epsino邻域的W(Sij<某个epsino时都置为0)
有两种K近邻。  K近邻和互K近邻（双边的）。  把单边的近邻就视为近邻的简称为K近邻。
一般都用K近邻图，需要把不对称的给调节成对称的
标签传递算法 LPA。它是半监督的。
K 均值不一定分对，在分之前，可能稍微对数据做一点处理就能起到好的效果，比如把数据旋转一下，再旋转回来。比如做个PCA。
方差不相等，数量不相等的K均值算法不一定合适。
向量的量化最常用的就是聚类。（矢量量化）
EM算法，期望最大化算法，也是无监督的。
PLSA也是用EM算法来推导的。
K 均值没法给出样本属于哪个簇的后验概率。
二维正态分布看我收藏那个百度那个就行了。
在EM算法的推导过程中，要使jenson不等式去等号，这时候不看第一个求和符号，只有 f(2/1*x1 + 2/1*x1)=2/1(f(x1) + f(x1)), 也就是必须要让自变量取一个和
x无关的常数才能取等号。具体在这个式子中，对于每一个i，都要取等号，对于每一个i，后面的加和符号是对z加和，z是自变量，而xi 是常数。所以让p(x,z,sita)/q(z)是常数就是让这个比值中没有z，
只有x,sita,因此就让这个比值等于 p 把z积掉就行了。
在EM算法中，E-step是固定了sita，求z，在E步骤中，sita是先验的给定的，也就是猜的。 M-step是固定了z，求sita。
在EM算法中应用Jenson不等式的时候，等固定sita的时候，Q取条件概率的时候不等式可以取等号。当sita发生变化时候，这时候Q中的sita还是之前没有变化的sita，所以这时候log后面的自变量发生了变化，就不是相等的了，这时候
jenson不等式取大于号，所以是满足之前找一个r(sita),在某个sita处和原函数相等，其他sita出都小于原函数，求r(sita)的思路的。
EM算法可以看做是坐标上升，不过老师那个图画的不对。应该是个三维图， 平面上对应的坐标为 sita 和 q， q不变，沿着sita上升一点， sita不变，沿着q上升一点，不断的迭代。
EM算法：隐变量在样本给定之下的条件概率，在这样的条件概率之下对联合概率的对数求期望，所有样本加和，作为我们的目标函数，求这个期望的最大值。J(sita)=sigma i  sigma z p(z|x,sita)ln(p(x,z,sita)).
LR 的坐标上升代码怎么写。	
PLSA可以看做是矩阵分解。EM算法是概率性的，PLSA看做矩阵分解是确定性的。EM算法可以用来学习PLSA的参数
pLSA中关心的就是每篇文章中的每个主题的分布，和每个主题下单词的分布。我们已经假设这两个分布为多项分布了，我们可以利用这两个分布做很多事情，比如文章分类，热词提取等等。
PLSA中，用EM算法来求 主题分布还有词分布。E步是求 z 的后验概率。
如果文档数目和词的数目不是特别多，没法用LDA的时候可以试试PLSA。
稀疏子空间聚类。
EM算法解决了无监督学习类别的估计。
在实际中，往往用 tied或者full。
AIC，BIC。  AIC=-2lnL + 2k.
如果认为参数sita不是一个定值，而是一个服从某一个分布的随机变量。抛硬币是sita是定值，过河sita是变化的。这时候。我们的拟合函数就从f(x;sita)变成了 f(x|sita)，表示在给定这个sita下，x的分布，比如是个高斯分布。
频率学派认为系统的参数是未知的确定的定值。 贝叶斯学派认为参数是服从某一个分布的变量，这样贝叶斯学派就可以对参数做一个分布上的假定。
beta分布是二项分布的先验分布。dirichlet分布是多项式分布的先验分布。
DPGMM
伪计数
K近邻
SVM准确率高，但是训练时间慢。之前用SVM，现在用LR，softmax了。
相对熵是可以用来定义互信息的。 I(x,y)=D( p(x,y)|| p(x)*p(y))   互信息为0等价于 p(x)*p(y)=p(x,y)，也就是独立
maxp(Ai|D) = max[p(D|Ai)p(Ai)/p(D)]。我们考虑的是给定D的情况下，哪个Ai出现的可能性最大，这里面变化的是Ai,p(D)相对来说是定值。
有项无环图就是贝叶斯网络。无向图是马尔科夫网络。LDA就是一个贝叶斯网络，隐马尔科夫模型也是贝叶斯网络。
贝叶斯网络的有向无环图中的节点表示随机变量，他们可以是可观察到的变量，或隐变量，未知参数等。每个节点在给定其直接前驱时，条件独立于其非后继。
全连接贝叶斯网络是我们最不希望的模型。
贝叶斯网络求联合概率分布的那个大乘法要会。
马尔科夫网络属于贝叶斯网络。往往不会去做更高阶的马尔科夫网络。
条件独立：tail-to-tail,head-to-tail.  head to head在C未知的情况下是独立的。（阻断，有向分离）
无向图的节点和边什么的，就是那样定义的。就是那么个规则。写联合分布的时候不要考虑 中间没有边连接的节点之间的关系，他俩就是没关系。想想概率那个表也是知道的。一个节点的条件概率只受连接到它的边的节点控制。
PRML这本书
建立贝叶斯网络的方法。1.中看不中用，看两个变量之间是否有边，看时候可以在条件概率的| 后面删掉一个变量后的概率与之前相不相等，如果相等，就没有边，如果不相等，就有边。因为数据有了，这些概率都能知道。但是这样得到的
模型并不合适。2.有先验知识，就用先验知识和业务逻辑去建立贝叶斯网络，没有先验知识和业务逻辑再用数据去建立。数据可以帮着做一些微调
混合网络。离散和连续混合在一起。隐马尔科夫模型就是混合网络。
正态分布的积分也可以作为分布，也是S型曲线。
最大生成树，最小生成树，criscar算法去解决贝叶斯网络的环的问题，也就是最大权生成树MSWT算法，为了保证近似网络跟原始网络的相对熵是最小的。criscar算法用并查集非常方便。
朴素贝叶斯往往用在文本处理当中。文本处理中的特征比较多，做一些先验性的假设更为合适。
EM算法记住男女GMM那个例子就行了。
是用TF-IDF提取文档特征
朴素贝叶斯是基于“特征之间（再给定类别y的情况下）是独立的”这一朴素假设，应用贝叶斯定理的监督学习算法。
“给定样本的情况下，p(x1,x2,x3,...,xn)是常数” 意思是说， 比如这枪打中了，那p就是0.6，这枪没打中，p就是0.4，给定这个样本是打中还是没打中，p的数值就确定了。
拉普拉斯平滑P(x1|c1) = n1 + 1 / n + N. 也可以分子加上alpha分母加上alpha乘以n
由0/1 向量改成频数向量或TF-IDF向量。 (term frequency * ln(inverse document frequency))
TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
朴素贝叶斯是一个弱模型，不会过拟合很正常。特征比较多的时候，用朴素贝叶斯的假定，效果是不错的。
LDA是多项分布，不是多点分布。每一个文档在词上都是多点分布，那么多文档，就是一个多项分布。 注意两点分布（掷骰子）和二项分布（掷多次骰子）的区别。
变分贝叶斯EM（因为含有隐变量，所以可以考虑用EM），gibbs采样去做LDA。
朴素贝叶斯可以解决一部分文本分类。但是没法解决一词多义和多词一义的问题。不熟贝叶斯更像是词法分析，不是语义分析。而增加主题可以解决这个问题。
gama函数是阶乘在实数域的推广。
dirichlet分布到底是个啥
在贝叶斯概率理论中，如果后验概率p(sita|x)和先验概率p(sita)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布（先验后验都是对于参数sita来说的）。似然分布是可以定下来的。比如掷骰子服从两点分布，房价预测误差服从高斯分布
JNDI
等车时间是指数分布，站台人数是泊松分布。
二项分布和两点分布的形式都是 p(x|sita)=sita的x次方乘以(1-sita)的(1-x)次方。 只不过二项分布有个有个系数啥的，归一化，让加和为1，
它们的共轭先验分布都是Beta分布。
对sita的估计是： 通过似然函数概率和先验概率计算后验概率，对后验概率对sita球期望就是对sita的估计值(这是简单的做法，其实应该求使似然函数取最大值的sita值)。
参数alpha,beta是决定参数sita的参数
多项分布的共轭先验分布是dirichlet分布。
beta分布求期望是 alpla/(alpha+beta), dirhichlet分布求期望是E(p1)= alpha1/(alpha1+...+alphak)
单纯形是什么
alpha=beta=1的beta分布是在(0,1)的均匀分布。alpha1=alpha2=。。。=1，也是均匀分布。
往往取alpha值都相同，即对称dirichlet分布，因为也没有别的先验知识。如果有相关的先验知识，知道关于某个主题的概率对大一些
可以调大这个主题对应的响应alpha值。
对称dirichlet分布的分析： 1。alpha=1,退化为均匀分布。2.alpha>1,p1=p2=...=pk的概率增大。3.alpha<1,某一个概率为1，其他概率为0的概率增大。
如果发现一片文档的分类结果是各个主题的概率都差不多，那应该调小alpha值，让主题更鲜明一些，即f在坐标轴上取值比较大。这是调参的方向。
word to vector
LDA 是要计算出主题分布和词分布。新文档的主题分布和词分布怎么 算？
由某一个先验分布得到一个主题分布，由某一个先验分布得到一个词分布，由主题分布得到主题，由主题去选择某一个词分布，
由词分布选择一个词，这就是LDA。它是一个典型的生成模型。z, phi,sit是要求的。
生成模型，判定模型。
LDA可以看成是降维，可以看成是聚类的过程。
LSI可以看成是矩阵分解。
隐马尔科夫模型有两个假设， 齐次假设和观测独立性假设。HMM有三个基本问题。概率计算问题，学习问题（状态未知用EM算法（在这里叫Baum-Welch算法），状态已知用最大似然估计），预测问题（viterbi算法，动态规划）。
终止字，非终止字。single，begin，middle，end。
前向算法和后向算法的时间复杂度是o(N平方*T)
维特比算法可以看做是动态规划。
走棋盘的最小路径算法问题，可以写出状态转移方程。
隐马模型和LDA都是生成模型。SVM和logistics回归都是判别模型。朴素贝叶斯是生成模型。
HMMlearn,Jieba
TensorFlow 也把复杂的计算放在 python 之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorflow 不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在 Python 之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）
一个非常常见的，非常漂亮的成本函数(损失函数)是“交叉熵”（cross-entropy）。
TensorFlow 在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法
TensorFlow 是一个编程系统, 使用图来表示计算任务。图中的节点被称之为 op (operation 的缩写)。 一个 op 获得 0 个或多个Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].

一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象； 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.
TensorFlow 程序使用 tensor 数据结构来代表所有的数据，计算图中, 操作间传递的数据都是 tensor. 你可以把 TensorFlow tensor 看作是一个 n 维的数组或列表. 一个 tensor 包含一个静态类型 rank, 和 一个 shape.
通常会将一个统计模型中的参数表示为一组变量。例如, 你可以将一个神经网络的权重作为某个变量存储在一个 tensor 中。在训练过程中, 通过重复运行训练图, 更新这个 tensor.
相对熵（relative entropy）又称为KL散度（Kullback–Leibler divergence，简称KLD），信息散度（information divergence），信息增益（information gain）。
 使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。
 dropout
 参数模型和非参数模型的区别
 SVM 的损失函数
 Logistics损失函数就是负对数似然
 决策树的损失函数就是评价函数
 剪枝和adaboost和hinge损失那（SVM里），adboost更新样本权值的依据需要看视频，EM  算法还得看一下。特征的等高线是什么？就是损失函数等高线，比如有两个特征的话，函数值是损失函数值L，自变量是两个特征。
 adaboost的损失函数是指数函数。
 GBDT还得找个博客看看
 所有公式推导得会推导
 
 
 
 作者：知乎用户
链接：https://www.zhihu.com/question/20446337/answer/74468226
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

最后说一说两种模型的优缺点：生成模型：优点：1）生成给出的是联合分布，不仅能够由联合分布计算条件分布（反之则不行），还可以给出其他信息，比如可以使用来计算边缘分布。如果一个输入样本的边缘分布很小的话，
那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的outlier detection。2）生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。
3）生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。缺点：1）天下没有免费午餐，联合分布是能提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，
需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到，因而如果我们只需要做分类任务，就浪费了计算资源。2）另外，实践中多数情况下判别模型效果更好。判别模型：优点：1）与生成模型缺点对应，
首先是节省计算资源，另外，需要的样本数量也少于生成模型。2）准确率往往较生成模型高。3）由于直接学习，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。
缺点：1）是没有生成模型的上述优点。

LDA 是PLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词分布，只是用的参数推断方法不同，在PLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两个参数弄成随机变量，且加入dirichlet先验。


24.防止过拟合的方法
　　过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。
　　处理方法有：
a.早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练
b.数据集扩增：原有数据增加、原有数据加随机噪声、重采样
c.正则化
d.交叉验证
e.特征选择/特征降维

28.对于树形结构为什么不需要归一化？
　　数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。
　　另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法
对于看不见的，需要长期积累的，阶段性努力，要重视，要当成是会立竿见影的事来做。
logistics需要对特征进行离散化？
条件随机场
I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义
互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示
41.线性分类器与非线性分类器的区别以及优劣
  如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
  常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归
  常见的非线性分类器：决策树、RF、GBDT、多层感知机
  SVM两种都有(看线性核还是高斯核)
  线性分类器速度快、编程方便，但是可能拟合效果不会很好
  非线性分类器编程复杂，但是效果拟合能力强

运行速度 存储效率 适用场合
数组 快 高 比较适合进行查找操作，还有像类似于矩阵等的操作
链表 较快 较高 比较适合增删改频繁操作，动态的分配内存
队列 较快 较高 比较适合进行任务类等的调度
栈 一般 较高 比较适合递归类程序的改写
二叉树（树） 较快 一般 一切具有层次关系的问题都可用树来描述
图 一般 一般 除了像最小生成树、最短路径、拓扑排序等经典用途。还被用于像神经网络等人工智能领域等等。
朴素贝叶斯那个模型
特征选择的标准方法
红黑树，作为一棵二叉查找树，满足二叉查找树的一般性质。下面，来了解下 二叉查找树的一般性质。
  二叉查找树，也称有序二叉树（ordered binary tree），或已排序二叉树（sorted binary tree），是指一棵空树或者具有下列性质的二叉树：
  若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值；
  若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值；
  任意节点的左、右子树也分别为二叉查找树。
  没有键值相等的节点（no duplicate nodes）。
  因为一棵由n个结点随机构造的二叉查找树的高度为lgn，所以顺理成章，二叉查找树的一般操作的执行时间为O(lgn)。但二叉查找树若退化成了一棵具有n个结点的线性链后，则这些操作最坏情况运行时间为O(n)。
  红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加了着色和相关的性质使得红黑树相对平衡，从而保证了红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。
  但它是如何保证一棵n个结点的红黑树的高度始终保持在logn的呢？这就引出了红黑树的5个性质：
  每个结点要么是红的要么是黑的。

  根结点是黑的。

  每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。

  如果一个结点是红的，那么它的两个儿子都是黑的。

  对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。
  正是红黑树的这5条性质，使一棵n个结点的红黑树始终保持了logn的高度，从而也就解释了上面所说的“红黑树的查找、插入、删除的时间复杂度最坏为O(log n)”这一结论成立的原因。更多请参见此文：http://blog.csdn.net/v_july_v/ ... 05630
也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。
  压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。
红黑树
为什么牛顿法是二阶收敛
拟牛顿法，共轭梯度法
最小二乘法又称为最小平方法，是一种数学优化技术，它通过最小化误差的平方和寻找数据的最佳函数匹配
KL变换和PCA
kmeans的复杂度
随机森林如何评估特征的重要性
KD-tree, ball tree
如何进行特征选择

精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）
　　召回率 recall = TP/(TP+FN) = TP/ P
　　F1值： 2/F1 = 1/recall + 1/precision
　　ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N


AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC。所以AUC反应的是分类器对样本
的排序能力。根据这个解释，如果我们完全随机的对样本分类，那么AUC应该接近0.5。另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因

可以直接优化AUC来训练分类器吗？答案是肯定的，我在这里给出一个简单的例子，通过直接优化AUC来训练一个逻辑回归模型。大家应该知道逻辑回归通常是通过极大似然估计来训练的，也就是最大化极大似然函数，这是统计里的概念，在机器学习里，对应logloss函数。我们通过下面的例子来训练一个逻辑回归是的AUC最大化
133.SVD和PCA
　　PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向
134.数据不平衡问题
　　这主要是由于数据分布不平衡造成的。解决方法如下：
  1）采样，对小样本加噪声采样，对大样本进行下采样
  2）进行特殊的加权，如在Adaboost中或者SVM中
  3）采用对不平衡数据集不敏感的算法
  4）改变评价标准：用AUC/ROC来进行评价
  5）采用Bagging/Boosting/ensemble等方法
  6）考虑数据的先验分布

SVM中有加权？

176.常见的分类算法有哪些？
　　SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

177.常见的监督学习算法有哪些？
　　感知机、svm、人工神经网络、决策树、逻辑回归
A.AR模型　　B.MA模型　　C.ARMA模型　　D.GARCH模型

而最小化后验概率是朴素贝叶斯算法要做的
由此，精准率定义为：P = TP / (TP + FP)，召回率定义为：R = TP / (TP + FN)，F1值定义为： F1 = 2 P R / (P + R)
　　精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。

A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力
　　B.Hinge 损失函数，作用是最小化经验分类错误
　　D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习

199.隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ）
　　A.评估—前向后向算法　　B.解码—维特比算法　　C.学习—Baum-Welch算法　　

感知机

Fisher 准则 ：更广泛的称呼是线性判别分析（LDA）
Bagging与Boosting的区别：
取样方式不同。

    Bagging采用均匀取样，而Boosting根据错误率取样。
    Bagging的各个预测函数没有权重，而Boosting是有权重的。
    Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。

3、Bagging，Boosting二者之间的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.

3）预测函数：

Bagging：所有预测函数的权重相等.

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.

 

4、随机森林

随机森林是Bagging的一个扩展变体，除了样本集是有放回的采样外，属性集合也引入了随机属性选择.具体来说，传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分. 

    随机森林简单、容易实现、计算开销小.效果能使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升.	
	
	精准率定义为：P = TP / (TP + FP)
召回率定义为：R = TP / (TP + FN)
F1值定义为： F1 = 2 P R / (P + R)

常见的判别式模型有：


Logistic regression（logistical 回归）
Linear discriminant analysis（线性判别分析）
Supportvector machines（支持向量机）
Boosting（集成学习）
Conditional random fields（条件随机场）
Linear regression（线性回归）
Neural networks（神经网络）


常见的生成式模型有:


Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）
Hidden Markov model（隐马尔可夫）
NaiveBayes（朴素贝叶斯）
AODE（平均单依赖估计）
Latent Dirichlet allocation（LDA主题模型）
Restricted Boltzmann Machine（限制波兹曼机）


生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。

A.最小最大损失准则

B.最小误判概率准则

C.最小损失准则

D.N-P判决

EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

前向后向算法：用来算概率

极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。
--------------------- 


解析：
Ｌ１范数具有系数解的特性，但是要注意的是，Ｌ１没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。

在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。因此选择A
--------------------- 
解析：

L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.

L2主要功能是为了防止过拟合，当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。

L1正则化/Lasso
L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

L2正则化/Ridge regression
L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。

可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。

因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。
--------------------- ！

势函数法

CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢

CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较

同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较

CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较
--------------------- 

K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。

首先线性的PCA（主成分分析法）和LDA（线性判别法）是肯定要知道的。

A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。

C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；

D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。


    6.如果线性回归模型中的随机误差存在异方差性，那么参数的OLS估计量是（）
    A. 无偏的，有效的
    B. 无偏的，非有效的
    C. 有偏的，有效的
    D. 有偏的，非有效的

参考答案：B


解析：对于分类器，主要的评价指标有 precision， recall，F-score，以及 ROC曲线等。在二分类问题中，我们主要关注的是测试集的正样本能否正确分类。当样本不均衡时，比如样本中负样本数量远远多于正样本，此时如果负样本能够全部正确分类，而正样本只能部分正确分类，那么( TP+ TN)可以得到很高的值，也就是Accuracy是个较大的值，但是正样本并没有取得良好的分类效果。当样本不均衡时，建议采用BCD方法来评价。
解析：这里是各种距离的特性的考察。
欧氏距离（Euclidean distance）也称欧几里得度量、欧几里得度量，是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离。在二维和三维空间中的欧氏距离的就是两点之间的距离。

特性：
平移不变性
旋转不变性

马氏距离(Mahalanobis distance)是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。为两个服从同一分布并且其协方差矩阵为Σ的随机变量与的差异程度:

如果协方差矩阵为单位矩阵,那么马氏距离就简化为欧氏距离,

如果协方差矩阵为对角阵,则其也可称为正规化的欧氏距离。

它是一种有效的计算两个未知样本集的相似度的方法。对于一个均值为μ，协方差矩阵为Σ的多变量向量，样本与总体的马氏距离为(dm)^2=(x-μ)’Σ^(-1)(x-μ)。

在绝大多数情况下，马氏距离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。

特性：
不考虑量纲影响（尺度不变性）
排除变量之间的相关性影响。（考虑了模式的分布）

A. 主成分分析PCA
B. 线性判别分析LDA
C. 深度学习SparseAutoEncoder
D. 矩阵奇异值分解SVD 	

SVM用径向基核时的gamma参数

所谓的前向特征选择法看起来很高大上，其实就是特征一个一个取，第一次取特征1，第二次取特征12，第三次取特征123…在试验中常用的方法。
而后向特征排除法也一样，第一次取123，第二次取12，第三次取1这种。
SVM中的c是什么

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

解析：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。过拟合的情况下则是低偏差，高方差。
多元共线，可以理解为多元变量呈线性关系

引入异步servlet的目的就是将容器线程池和业务线程池分离开。在处理大io的业务操作的时候，把这个操作移动到业务线程池中进行，释放容器线程，使得容器线程处理其他任务，在业务逻辑执行完毕之后，然后在通知tomcat容器线程池来继续后面的操作，这个操作应该是把处理结果commit到客户端或者是dispatch到其他servlet上。

剖析一个算法的时候，可以尝试增加或者减弱一个前提条件（增强或者减弱假设，一步步得出结论） 看看这个算法是怎么回事。 可以用这个思路想想 KMP算法是怎么想来的。

KMP没有退化情况，最好最差都是0(N), 快排的平均和最好是是nlogn,最差是n方
是用经典KMP算法计算字符串的最小周期。（是用未改进的KMP算法的next数组就可以）
问题规模会导致算法的不同。海量数据可以用空间换时间。 比如用trie树，hash。
BM算法是更实用的线性时间复杂度算法。
广度优先搜索，深度优先搜索
数组，通过下标可以通过O(1)来找到某元素。若元素之间无序，可以通过O(N)来确定元素是不是在数组中。若有序，则可以使用折半查找确定某元素是不是在数组中，需要O(nlogn)的时间复杂度。（写一个出来传到github上）
课件中众数那有问题
求局部最大值
第一个缺失的整数
查找旋转数组的最小值
零子数组
最大连续子数组，类似于隐马尔科夫模型，有前向算法和后向算法，可以求以a[i]结束的最大子数组，也可以求以a[i]开始的最大子数组的和。如果还想要知道最大子数组包含哪些元素呢，那就再存一个from和to
数字连续的子数组。
荷兰国旗问题，荷兰问题的reverse版本，除了借鉴快排的思路，也可以用循环不变式
最短路径的地杰斯特拉算法
快速排序中的partition思路，定义一个begin，current和end
奇偶排序，正负排序
荷兰国旗问题可以使快速排序最差的时间复杂度也是nlogn
logistics回归虽然叫回归，但是跟回归是两码事，是做分类的。 二叉树和树的概念也不同。
树转换成二叉树的时候，可以用左孩子，右兄弟的方式，当然也可以用这种方法把森林转换成二叉树。
写一个树和链表
二叉查找树查找某一个数据的code，插入数据的code。
Btree，数据库索引一般用btree
treemap
二叉树的插入可以有两种写法，递归和非递归，都写一下。
二叉树的删除
直接后继是右孩子的最左分支，直接前驱是左孩子的最右分支。
线索树： 节点为N的二叉树会浪费 2N-(N-1)=N+1个指针，可以用这些指针指向节点的后继。（TBC）
huffman编码，它是前缀编码，不需要分隔符就能够做分割。它是最优的无损压缩，它是一个贪心算法。huffman编码的结果并不唯一，谁给0谁给1不一定，频数相同的元素在做合并时也不一定。huffman编码形成的二叉树的节点只能有
两个节点或者没有节点。看下老师写的代码，是用数组来存的一棵树，怎么弄的。
课件到时候看一下。
递归方式和用栈的方式 两种方式写出 树的中序，前序和后续遍历。可以用python写。非递归（中序和后序）的写法还是有点难的，后面看一下。
其实非递归并没有比递归好多少，因为都是用的栈，只不过非递归我们自己写栈，递归，编译器给我们默认用栈。
前序，中序和后序本质上都是图这种数据结构的深度优先搜索。树的广度优先都说对应的是层序遍历。
已知二叉树的前序，中序遍历，求它的后序遍历。可以用递归来绕开生成这个二叉树。当然也可以用中序，后序求前序。给定前序遍历和后序遍历是无法唯一确定一颗二叉树的，除非多给点条件。
给定一个数组，这个数组从小到大排列就是一个二叉搜索树的中序遍历结果。
寻找最大的二叉搜索子树。
二叉树的翻转，递归
所有括号匹配的字符串
k个不同字符的最长字串 （map可以认为是红黑树）
2-3-4树可以方便的转化成红黑树
b树，b+树。
K近邻的空间复杂度太高，在搜索近邻时候太慢，所以可以用kd-tree来帮助干这个事，具体怎么干的，还得看看
树是数据结构，并非存储结构，实践中可以使用数组来存储树。
树 在实践中，往往作为海量数据索引。
数据结构之逻辑结构与物理结构（存储结构）
存储结构 和数据结构 的概念是什么
map的后台是红黑树实现的。
红黑树是一种结点带有颜色属性的二叉查找树，但它在二叉查找树之外，还有以下要求：

    节点是红色或黑色。
    根是黑色。
    所有叶子都是黑色（叶子是NIL节点）。
    每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。）
    从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。
而红黑树由于在插入和删除结点时都会进行变色旋转等操作，在符合红黑树条件的情况下，即使一边子树全是黑色结点，另一边子树全是红黑相间，两子树的高度差也不会超过一半。一棵有 n 个结点的红黑树高度至多为 2log(n+1)，查找效率最坏为 O(log(n))。

所以红黑树常被用于需求查找效率稳定的场景，如 Linux 中内核使用它管理内存区域对象、Java8 中 HashMap 的实现等，
2-3-4树实现平衡是通过结点的旋转和结点元素数变化，红黑树是通过结点旋转和变色。
B树是为实现高效的磁盘存取而设计的多叉平衡搜索树
B树是用最小度来定义的。某个节点最多含有2t-1个关键字，除根节点外的每个节点至少有t-1个关键字，根节点至少有一个关键字。
五、从B树删除关键字
删除操作的基本思想和插入操作是一样的，都是不能因为关键字的改变而改变B树的结构。插入操作主要防止的是某个节点中关键字的个数太多，所以采用了分裂；删除则是要防止某个节点中，因删除了关键字而导致这个节点的关键字个数太少，所以采用了合并操作。
下面总结一下B树的删除原理：

    基本原则是不能破坏关键字个数的限制；
    如果在当前节点中，找到了要删的关键字，且当前节点为内部节点。那么，如果有比较丰满的前驱或后继，借一个上来，再把要删的关键字降下去，在子树中递归删除；如果没有比较丰满的前驱或后继，则令前驱与后继合并，把要删的关键字降下去，递归删除；
    如果在当前节点中，还未找到要删的关键字，且当前节点为内部节点。那么去找下一步应该扫描的孩子，并判断这个孩子是否丰满，如果丰满，继续扫描；如果不丰满，则看其有无丰满的兄弟，有的话，从父亲那里接一个，父亲再找其最丰满的兄弟借一个；如果没有丰满的兄弟，则合并，再令父亲下降，以保证B树的结构。
图的存储结构： 邻接矩阵（比较耗费内存，n个节点需要n方的空间），或者邻接表
最为基本的图的搜索方式， 广度优先搜索，深度优先搜索
并查集存储的是父节点在哪
数据结构的基本操作决定了它的应用范围，对并查集而言，一个简单的应用就是判断无向图的连通分量个数，或者判断无向图中任何两个顶点是否连通。
db-scan可以用并查集来实现
强连通的概念
并查集可以用数组实现
计算割点
DFS，深度优先搜索， BFS，广度优先搜索
最小生成树算法
dijkstra算法其实是一个贪心算法。是广度优先搜索。n方的复杂度，在实际应用中是不堪用的。实际应用中往往用A*算法。dijkstra算法要求边的权值为正，可以求出一个点到任意节点的最短路径。
dijkstra算法的堆优化怎么弄
算法的一个套路就是，要想求出 解 在哪，（比如最短路径本身是在哪取得的），就记录一下前驱。
在一个数据结构中找出最小值，其实没有必要遍历所有值。用堆就可以解决，所以在dijkstra算法中，没有必要用数组来存，用堆的话就可以保证在logN的时间复杂度下找到最小值，以logN的时间复杂度调整堆
启发式搜索，A*算法
Floyd算法，又称为插点法，用于计算图中任意两点的最短路径。并且不要求边的权值为正。时间复杂度是n的立方。特别像矩阵乘法。如果想把最短路径是什么求出来，就记录前驱或者后继就可以。
Floyd算法本质上是动态规划算法。Bellman-ford也是动态规划算法。
(1)Floyd 可以有负权边是因为它依靠的动态规划，比如a-b权值为1,而a-c权值2,c-b权值为-3，那么根据算法a-b最短路径为-1.Dij算法不能有负权边的原因是它依靠贪心算法，a-b最短路径就为1,实际上是-1
(2)Floyd 不能有负权回路，这个容易理解，a-b，b-c，c-a权值分别为1，-2，-3，那么一直这样回路下去a-b-c-a会一直小，显然算法要在这儿停下，不然就没最短路径这一说
 dijkstra,Bellman_Ford,Floyd算法的比较:
：Dijkstra算法,图所有边权值都为非负的;
：Bellman_Ford算法,图中所有边权值可以存在负值,但是不能存在原点可达的负权回路,如果存在负权回路,该算法可以给出判断;
：Floyd算法,不允许所有权值为负的回路,可以求出任意两点间的最短距离,而Dijkstra和Bellman_Ford算法只可以求出任意点到达源点的最短距离;
：Dijkstra算法的思想是贪心,Bellman_Ford和Floyd算法的思想是动态规划
：三者:图中都可以出现正权回路
 Dijkstra, SPFA, floyd, Bellman-Ford
 bellman-ford算法也是单源点最短路径算法。
 最小生成树MST算法最著名的两个算法是 Prim算法和Kruskal算法，他们本质都是贪心算法。Kruskal算法更简单，直观。并查集可以帮我们快速看一看两个节点是不是构成了环。
 拓扑排序是最重要的判断是否有环的方法。
 Chow-Liu算法是 Kruskal算法的应用
 广度优先搜索，BFS可以用队列实现
 单词变换问题，world ladder, 是求单源点最短路径问题，本质就是dijkstra问题，需要把隐式图画出来。其实做的时候没有必要把整个图都建立起来，可以边扩展边去生成这个图。
 最最短啊，最小啊，最少啊，往往可以是用BFS，这是套路，但不一定是。可以用队列
 周围区域问题
 DFS是 走到头再回溯。 一般暴力枚举是DFS问题。DFS的实现要么用递归，要么自己是用堆栈，其实都一样，递归也是是用栈递归的。
 DFS做到一半的时候，会把数据分为三个集合。完全知道信息的，知道部分信息的，完全不知道信息的。  w,g,b。
老鼠吃奶酪问题。如果是找一条最短路径，那一般做一个广度优先搜索，谁先吃到9，那做一个从后到前找前驱的过程。找一条路径的话，DFS比BFS简单的多，套路是一样的。DFS方便用递归，BFS不方便用递归。
 DFS，BFS写法框架要会写。
 百数问题，暴力和DFS。
 八皇后问题
 数独问题
 递归是实现手段，DFS是理论依据。其实他俩没那么大区别
 蚁群算法是什么，没有讲。
 归并排序
归并排序改进。1，可以不存数据本身，存索引，也就是存数组下标。2，归并排序，在数组长度比较短的情况下，可以
不尽兴递归，也就是不递归到底，而是选择其他排序方案，如插入排序。
基于关键字比较的排序算法的平均时间复杂度的下界为O(nlogn).
归并排序是稳定的排序，堆排序和快排是不稳定的排序。
空间复杂度，堆排序是原地排序。快排如果考虑递归栈的话是logN，归并至少是o(N).
时间复杂度,归并排序最好最差平均都是nlogn，堆排序的平均最好最差也是nlogn，快排平均和最好都是nlogn，最差时间复杂度是n方
堆排序
in-place 归并排序是什么
归并排序是适合做外排序的。 在 i，j两个下标之后的数组中的数据没必要都读进内存中。外排序通常将中间结果放在读写较慢的外存储器上（通常是硬盘），外排序通常采用“排序——归并”的策略。排序阶段，读入能放在内存中的数据量，将其排序输出到临时文件，依次进行，将待排序数据组织为多个有序的临时文件。归并阶段，将这些临时文件组合为大的有序文件。
快速排序和堆排序是要求数据必须在内存当中的。
外排序举例，100M内存来给900M数据排序。（归并排序中，排序的部分其实不一定非得用归并排序去递归，用其他排序算法也可以。（比如堆排序））。9路归并排序的code写一下
map－reduce的思路是什么
逆序数问题，求出逆序数对数和逆序对都是什么。n方的写法是基本写法。nlogn的写法，有个对偶写法，都写一下。
杨氏矩阵的增删查改，只要行列都是有序的就行，不一定非得都是生序。插入：教材的写法是让数据形成一个上三角矩阵。查找的时候可以把数据定义在左下角或者右上角（这是对于行列都是生序的杨氏矩阵的）。删除是插入的反过程。改的话，是先给这个元素删了，然后再插入要改成的那个元素。
迷离傍地走，4个进程，两个cpu，怎么安排这四个进程的使用顺序，使这四个进程完成的时间最短。使用贪心法就可以。这是两个pipeline的进程调度问题。
寻找和为定值的两个数字，暴力解法和稍微好一点的解法。2-sum问题。
3-sum问题。用sum－a［i］，然后剩下的n－1的元素找两个元素的和是a［i］，变成了2-sum问题。
N－sum问题。取出若干个值，它们的和是定值。code在数组那次的附录里。用Hash。
在实际当中，要想查找某一个值在哪，就是用hash，是最简单的方法。hash的缺点就是空间复杂度，但是空间复杂度不在乎了。
在字符串的查找中，键树比hash还快。
Eratosthenes筛
一般来说，如果在排序过程中，是相邻元素的比较，是稳定的，如冒泡，归并。如果间隔元素进行了比较，往往是非稳定的，如堆排序，快速排序。如果问归并是不是稳定的，得答不一定，因为归并中可在排序部分可以用其他算法来排序。
加上key,index，可以使不稳定排序变成稳定排序。
 计数排序，本质是用空间换时间，本质是建立了基于元素的Hash表。它是一个线性时间的排序算法。
 如果想找到流式数据的中值，或者找到第100大的数，第10大的数，这种问题，可以用堆。或者如果可以拿到所有数据的话，可以用BFPRT算法，只不过要求数据都能进内存，可以o(n)，如果进不到内存的话，就只能用堆了。
 优先队列。
 基于关键字比较的排序，下限就是nlogn.，如果说想用线性时间复杂度搞定，那就是不基于关键字比较，那都是显式或者隐式的转换成为计数排序的思想。
 桶排序/基数排序,可以不受nlogn下限的影响，如果桶里面只有一个元素的话，就退化成了计数排序。
 排序的目的，排序本身还有方便查找。长度为N的有序数组，查找元素时间复杂度为o(logn). 长度为N的有序链表，不管是单链表还是双链表，查找某一个元素的时间复杂度为O(N)，因为链表是无法做随机索引的，无法通过low 和high
 找到 middle。因此可能使用跳链表来搞定。
 计数排序从后往前是为了稳定性
 锦标赛排序，选最大和次大的例子，只需要logN 次比较。
 当数列取值范围过大或者数组不是整数时，可以用桶排序来解决。 极端情况下，数据分布特别不均匀，可能有很多空桶，而个别桶中数据很多，就汇造成时间复杂度退化成nlogn. （m=n的话时间复杂度是o(N)）
 排序算法的选择应该是根据内存，cpu，时间空间来选择。机器学习算法的选择是根据数据来选择。
 同时求出一个数组的最大最小值很容易。 先另最大最小值max，min，都为首个元素，然后遍历数组，遇到小的就给 min变量，遇到更大的就给max变量，遍历数组比较 a[i]和 min，max的大小就行。时间复杂度为o(N)
 冒泡的时间复杂度是n方，是上限
做一个拓扑排序，如果输出结果的有效元素个数是n个，那说明这个图是没有环的。如果原图有环，那输出的拓扑排序的个数小于n，所以可以用拓扑排序算法来确定一个图是不是有环。而且没必要非要用队列，堆栈，数组什么都可以。不一定要先如先出。
权值相同的最短路径问题，Dijkstra算法就退化成了BFS广度优先搜索。Dijkstra算法是单源点的。
以队列作为数据结构，所得到的图的遍历方式，一般都是广度优先搜索，BFS。
空间换时间的方法，待续
海量数据的字符串查找，往往需要hash表，千万别用KMP。比如在十亿个URL中查找某URL出现的位置，就用hash，拿到一个URL就hash一下，拿到一个url就hash一下，冲突了久搞个尾巴这样来做。
比如比较两个文本的相似度，可以用LCS来看，但是如果文本巨大，那就做两个辞典，搞出两个向量，词出现了为1，不出现为0，计算这两个向量的夹角余贤。问题的规模会导致算法的不同。
实际上并不用KMP，而是用BM算法。
树和图之类的问题，都是有套路的。
StringBuilder 和 StringBuffer的区别，statement 和prepare statement的区别
树可以通过左孩子，友兄弟的方式转换成二叉树，多颗树组成的森林也可以通过这种方式转换成二叉树，每棵树的根节点都当成是兄弟就可以了。
二叉查找树（二叉搜索树）：左子树上的所有节点都小根节点，右子树所有节点都不小于根节点。所有子树也满足这个条件。
huffman编码是无损压缩中最优的，没有之一。huffman编码是一种贪心算法。huffman不是唯一的，左0右1或者左1右0都可以，而且如果要编码的元素的频数相同，这两个元素可以调换位置。
不等长编码的译码必须满足前缀编码的条件：任何一个字符的编码都不是另外一个字符编码的前缀。huffman树中的节点要么有两个节点，要么没有节点。中间节点
都是内部节点，待编码的元素都在叶子结点上。
优先队列就是堆。二叉树的升序遍历本质上就是数据的升序过程。不管前序，中序还是后序都是图的深度优先搜索，广度优先搜索，对应树的是它的层序遍历。
树，根据前序遍历和中序遍历，是可以唯一确定这棵树的。中序和后序也是可以唯一确定一颗二叉树的。前序和后序不能唯一确定
map可以认为是红黑树。红黑树的理论基础是2-3-4树。2-3-4树可以很方便的转换成红黑树红节点的孩子必须黑，黑节点的孩子有可能红，有可能黑。从根节点到叶子节点，黑色节点的路径长度都是一样的。
最长路径和最短路径的比值是小于等于2的（包括红节点）。
对B树做一个变换，中间节点值存储索引，所有数据都存储在叶子节点上，就成了B+树。KD-tree. k近邻搜索任何一个节点的邻域的样本个数慢，所以可以用kd-tree,帮助做
索引。关系型数据库往往用B树做索引，而不用哈希，为什么？
树是数据结构，并非存储结构，逻辑上一棵树，可以用数组去存储它。
eclipse怎么判断一个应该有返回值的方法，会不会出现没有返回值这种错误。比如很多if else,怎么判断不管怎样都有返回值了。而且不会重复
要想记录具体的解是什么，就记录前驱
用堆这个数据结构可以用logn的时间复杂度找到最小值，可以用logn的时间复杂度调整堆，所以用堆来代替数组作为dijkstra的存储结构，时间复杂度就降下来了。
